{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"study_group[05].ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO85oR7vPoy4LsM7jle0nib"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 인트로\n","\n","- 0 : T-shirt/top\n","\n","- 1 : Trouser\n","\n","- 2 : Pullover\n","\n","- 3 : Dress\n","\n","- 4 : Coat\n","\n","- 5 : Sandal\n","\n","- 6 : Shirt\n","\n","- 7 : Sneaker\n","\n","- 8 : Bag\n","\n","- 9 : Ankle boot"],"metadata":{"id":"0ig_xCSkus1Z"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"I9nZom2f-yi0","executionInfo":{"status":"ok","timestamp":1646352576818,"user_tz":-540,"elapsed":2673,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import pandas as pd\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from tensorflow.keras.models import *\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from tensorflow.keras.regularizers import *\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import StratifiedKFold"]},{"cell_type":"markdown","source":["# 1.데이터 불러오기"],"metadata":{"id":"CHr1YCoCwRJr"}},{"cell_type":"code","source":["train = pd.read_csv('train.csv').iloc[:, 1:]\n","test = pd.read_csv('test.csv').iloc[:, 1:]\n","submission = pd.read_csv('sample_submission.csv')\n","\n","display(train)\n","display(test)\n","display(submission)"],"metadata":{"id":"dAk3rWQxuNUu","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1646352580875,"user_tz":-540,"elapsed":4063,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"21eb0fa7-8d90-471b-90d1-72425840e447"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <div id=\"df-cb24cb06-1f52-48e2-aaaf-983915b15035\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>pixel9</th>\n","      <th>...</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","      <th>pixel784</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30</td>\n","      <td>43</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>59995</th>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59996</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>73</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59997</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>160</td>\n","      <td>162</td>\n","      <td>163</td>\n","      <td>135</td>\n","      <td>94</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59998</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59999</th>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>60000 rows × 785 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb24cb06-1f52-48e2-aaaf-983915b15035')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cb24cb06-1f52-48e2-aaaf-983915b15035 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cb24cb06-1f52-48e2-aaaf-983915b15035');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n","0          2       0       0       0       0       0       0       0       0   \n","1          9       0       0       0       0       0       0       0       0   \n","2          6       0       0       0       0       0       0       0       5   \n","3          0       0       0       0       1       2       0       0       0   \n","4          3       0       0       0       0       0       0       0       0   \n","...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n","59995      9       0       0       0       0       0       0       0       0   \n","59996      1       0       0       0       0       0       0       0       0   \n","59997      8       0       0       0       0       0       0       0       0   \n","59998      8       0       0       0       0       0       0       0       0   \n","59999      7       0       0       0       0       0       0       0       0   \n","\n","       pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n","0           0  ...         0         0         0         0         0   \n","1           0  ...         0         0         0         0         0   \n","2           0  ...         0         0         0        30        43   \n","3           0  ...         3         0         0         0         0   \n","4           0  ...         0         0         0         0         0   \n","...       ...  ...       ...       ...       ...       ...       ...   \n","59995       0  ...         0         0         0         0         0   \n","59996       0  ...        73         0         0         0         0   \n","59997       0  ...       160       162       163       135        94   \n","59998       0  ...         0         0         0         0         0   \n","59999       0  ...         0         0         0         0         0   \n","\n","       pixel780  pixel781  pixel782  pixel783  pixel784  \n","0             0         0         0         0         0  \n","1             0         0         0         0         0  \n","2             0         0         0         0         0  \n","3             1         0         0         0         0  \n","4             0         0         0         0         0  \n","...         ...       ...       ...       ...       ...  \n","59995         0         0         0         0         0  \n","59996         0         0         0         0         0  \n","59997         0         0         0         0         0  \n","59998         0         0         0         0         0  \n","59999         0         0         0         0         0  \n","\n","[60000 rows x 785 columns]"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <div id=\"df-01c38cb6-edc5-4376-adc5-6d72b2c5fb34\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>pixel9</th>\n","      <th>pixel10</th>\n","      <th>...</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","      <th>pixel784</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>103</td>\n","      <td>87</td>\n","      <td>56</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>14</td>\n","      <td>53</td>\n","      <td>99</td>\n","      <td>17</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>63</td>\n","      <td>53</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>161</td>\n","      <td>...</td>\n","      <td>137</td>\n","      <td>126</td>\n","      <td>140</td>\n","      <td>0</td>\n","      <td>133</td>\n","      <td>224</td>\n","      <td>222</td>\n","      <td>56</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>37</td>\n","      <td>...</td>\n","      <td>32</td>\n","      <td>23</td>\n","      <td>14</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>52</td>\n","      <td>23</td>\n","      <td>28</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>175</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>182</td>\n","      <td>199</td>\n","      <td>222</td>\n","      <td>42</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>140</td>\n","      <td>119</td>\n","      <td>103</td>\n","      <td>...</td>\n","      <td>111</td>\n","      <td>95</td>\n","      <td>75</td>\n","      <td>44</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 784 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01c38cb6-edc5-4376-adc5-6d72b2c5fb34')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-01c38cb6-edc5-4376-adc5-6d72b2c5fb34 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-01c38cb6-edc5-4376-adc5-6d72b2c5fb34');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n","0          0       0       0       0       0       0       0       9       8   \n","1          0       0       0       0       0       0       0       0       0   \n","2          0       0       0       0       0       0      14      53      99   \n","3          0       0       0       0       0       0       0       0       0   \n","4          0       0       0       0       0       0       0       0       0   \n","...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n","9995       0       0       0       0       0       0       0       0       0   \n","9996       0       0       0       0       0       0       0       0       0   \n","9997       0       0       0       0       0       0       0       0       0   \n","9998       0       1       3       0       0       0       0       0       0   \n","9999       0       0       0       0       0       0       0     140     119   \n","\n","      pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n","0           0  ...       103        87        56         0         0   \n","1           0  ...        34         0         0         0         0   \n","2          17  ...         0         0         0         0        63   \n","3         161  ...       137       126       140         0       133   \n","4           0  ...         0         0         0         0         0   \n","...       ...  ...       ...       ...       ...       ...       ...   \n","9995       37  ...        32        23        14        20         0   \n","9996        0  ...         0         0         0         2        52   \n","9997        0  ...       175       172       172       182       199   \n","9998        0  ...         0         0         0         0         0   \n","9999      103  ...       111        95        75        44         1   \n","\n","      pixel780  pixel781  pixel782  pixel783  pixel784  \n","0            0         0         0         0         0  \n","1            0         0         0         0         0  \n","2           53        31         0         0         0  \n","3          224       222        56         0         0  \n","4            0         0         0         0         0  \n","...        ...       ...       ...       ...       ...  \n","9995         0         1         0         0         0  \n","9996        23        28         0         0         0  \n","9997       222        42         0         1         0  \n","9998         1         0         0         0         0  \n","9999         0         0         0         0         0  \n","\n","[10000 rows x 784 columns]"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <div id=\"df-a7f3d577-9c1e-4297-99eb-ca638609b110\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>9995</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>9996</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>9997</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>9998</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>9999</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7f3d577-9c1e-4297-99eb-ca638609b110')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a7f3d577-9c1e-4297-99eb-ca638609b110 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a7f3d577-9c1e-4297-99eb-ca638609b110');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      index  label\n","0         0      0\n","1         1      0\n","2         2      0\n","3         3      0\n","4         4      0\n","...     ...    ...\n","9995   9995      0\n","9996   9996      0\n","9997   9997      0\n","9998   9998      0\n","9999   9999      0\n","\n","[10000 rows x 2 columns]"]},"metadata":{}}]},{"cell_type":"code","source":["print(train.info())\n","print()\n","print(test.info())"],"metadata":{"id":"fN35JRWUuNgk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646266620484,"user_tz":-540,"elapsed":14,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"cd28a069-9162-4106-d9e1-98466597852f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 60000 entries, 0 to 59999\n","Columns: 785 entries, label to pixel784\n","dtypes: int64(785)\n","memory usage: 359.3 MB\n","None\n","\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10000 entries, 0 to 9999\n","Columns: 784 entries, pixel1 to pixel784\n","dtypes: int64(784)\n","memory usage: 59.8 MB\n","None\n"]}]},{"cell_type":"markdown","source":["# 2.데이터 형태 변경"],"metadata":{"id":"dCzFP1iFB90F"}},{"cell_type":"code","source":["# 실수형으로 변환\n","X = np.array(train.drop('label', axis = 1), dtype = 'float32')\n","# y = train.label\n","target = np.array(test, dtype='float32')\n","display(X)\n","y = train.label\n","display(y)\n","target = np.array(test, dtype='float32')\n","display(target)\n","# 케라스는 0~1 사이의 값으로 구동시킬때 성능이 좋으므로\n","# 255로 나눠 정규화\n","X /= 255\n","target /= 255"],"metadata":{"id":"jC97dsoDCDNj","colab":{"base_uri":"https://localhost:8080/","height":492},"executionInfo":{"status":"ok","timestamp":1646266621045,"user_tz":-540,"elapsed":569,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"4c5a59c0-b349-4699-d66f-ccdd19e9754f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0        2\n","1        9\n","2        6\n","3        0\n","4        3\n","        ..\n","59995    9\n","59996    1\n","59997    8\n","59998    8\n","59999    7\n","Name: label, Length: 60000, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 1., 0.],\n","       [0., 1., 3., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"]},"metadata":{}}]},{"cell_type":"code","source":["# 샘플 출력\n","image = X[2022,:].reshape(28,28)\n","print(f'{int(y[2022])}번 클래스 의류')\n","plt.imshow(image)\n","plt.show()"],"metadata":{"id":"FCqrh_nJCLNk","colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"status":"ok","timestamp":1646266621046,"user_tz":-540,"elapsed":17,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"8aaa7377-af5a-4ea1-b98a-364904a76a63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5번 클래스 의류\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# 28,28,1로 지정\n","X = X.reshape(-1, 28, 28, 1)\n","target = target.reshape(-1, 28, 28, 1)\n","# 랜덤 시드 설정\n","np.random.seed(120)"],"metadata":{"id":"GcIDiE19CRAB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.딥러닝\n","\n"],"metadata":{"id":"xlzA_IUCCXH6"}},{"cell_type":"markdown","source":["### StratifiedKFold\n","- kfold가 치우쳐서 학습하는 단점을 해결하기 위해 사용한다.\n","- StratifiedKFold는 타겟에 속성값의 갯수를 동일하게 가져감으로 kfold처럼 데이터가 몰리는 것을 막아줌"],"metadata":{"id":"OlFMOPkNDv3n"}},{"cell_type":"markdown","source":["### ReduceLROnPlateau는 콜백 함수\n","- 모델의 개선이 없을 경우, Learning Rate를 조절해 모델의 개선을 유도하는 콜백함수"],"metadata":{"id":"_4K9qN0WDyay"}},{"cell_type":"code","source":["skf = StratifiedKFold(n_splits = 15, random_state = 1011, shuffle = True)\n","reLR = ReduceLROnPlateau(patience = 5,verbose = 1,factor = 0.5) # 학습률 조정\n","es = EarlyStopping(patience = 5, verbose=1, monitor = 'val_acc', mode = 'max') # 학습 조기 종료"],"metadata":{"id":"uNWqqpWtCRCn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub_pred = np.zeros((test.shape[0], 10))\n","print(sub_pred.shape)\n","\n","for i, (tr_idx, val_idx) in enumerate(skf.split(X, y)) :\n","  print('=' * 25)\n","  print(f'{i + 1}번째 학습 FOLD 학습 시작')\n","    \n","  tr_x, tr_y = X[tr_idx], y[tr_idx] # 학습데이터\n","  val_x, val_y = X[val_idx], y[val_idx] # 검증데이터\n","\n","# 모델을 저장할 때 사용되는 콜백함수\n","  mc = ModelCheckpoint(f'cv_study{i + 1}.h5',save_best_only=True, verbose=1, monitor = 'val_acc', mode = 'max', save_weights_only=True)\n","\n","  model = Sequential()\n","\n","  model.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1),padding='same'))  # 컨볼루션 레이어 , 여기서 padding : 경계 처리 방법이며‘same'는 출력 이미지 사이즈가 입력 이미지 사이즈와 동일\n","  model.add(BatchNormalization())  #  변형된 분포가 나오지 않도록 조절\n","  # https://eehoeskrap.tistory.com/430\n","  model.add(Dropout(0.3))  # 과대적합(Overfitting)을 방지하기 위해 사용\n","  model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n","  model.add(BatchNormalization())\n","  model.add(Conv2D(32,(5,5),activation='relu',padding='same')) \n","  model.add(BatchNormalization())\n","  model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n","  model.add(BatchNormalization())\n","  model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D((3,3))) # 컨볼루션 레이어의 출력 이미지에서 주요값만 뽑아 크기가 작은 출력 영상을 만들어 사소한 변화가 영향을 미치지 않도록 함\n","  model.add(Dropout(0.3))\n","  model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n","  model.add(BatchNormalization())\n","  model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D((3,3)))\n","  model.add(Dropout(0.3))\n","  model.add(Flatten()) # 2차원 배열을 1차원으로 바꿔줌\n","  model.add(Dense(128,activation='relu')) # 은닉층 128개\n","  model.add(BatchNormalization())\n","  model.add(Dropout(0.3))\n","  model.add(Dense(64,activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dropout(0.3))\n","  model.add(Dense(10,activation='softmax'))\n","\n","  model.compile(loss='sparse_categorical_crossentropy', optimizer = RMSprop(lr=0.003),metrics=['acc'])\n","  # https://ahnjg.tistory.com/88\n","\n","  history = model.fit(tr_x, tr_y, epochs = 1000, \n","                      validation_data = (val_x, val_y), callbacks = [es, mc, reLR])\n","\n","  model.load_weights(f'cv_study{i + 1}.h5')\n","\n","  pred = model.predict(target) / 15\n","  sub_pred += pred\n","  print(f'{i + 1}번째 학습 FOLD 학습 완료\\n')"],"metadata":{"id":"ozx50kXJCRH9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646194447003,"user_tz":-540,"elapsed":7955533,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"afe348d5-5109-4bd2-efd6-2de34dcb1df5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=========================\n","1번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.5537 - acc: 0.8036\n","Epoch 1: val_acc improved from -inf to 0.88250, saving model to cv_study1.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.5537 - acc: 0.8036 - val_loss: 0.3363 - val_acc: 0.8825 - lr: 0.0030\n","Epoch 2/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8755\n","Epoch 2: val_acc did not improve from 0.88250\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.3630 - acc: 0.8755 - val_loss: 0.3291 - val_acc: 0.8808 - lr: 0.0030\n","Epoch 3/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.3187 - acc: 0.8902\n","Epoch 3: val_acc improved from 0.88250 to 0.90800, saving model to cv_study1.h5\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.3187 - acc: 0.8903 - val_loss: 0.2458 - val_acc: 0.9080 - lr: 0.0030\n","Epoch 4/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.8999\n","Epoch 4: val_acc improved from 0.90800 to 0.91875, saving model to cv_study1.h5\n","1750/1750 [==============================] - 23s 13ms/step - loss: 0.2942 - acc: 0.8999 - val_loss: 0.2168 - val_acc: 0.9187 - lr: 0.0030\n","Epoch 5/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9068\n","Epoch 5: val_acc did not improve from 0.91875\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2769 - acc: 0.9066 - val_loss: 0.2307 - val_acc: 0.9170 - lr: 0.0030\n","Epoch 6/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9101\n","Epoch 6: val_acc improved from 0.91875 to 0.92050, saving model to cv_study1.h5\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.2610 - acc: 0.9101 - val_loss: 0.2231 - val_acc: 0.9205 - lr: 0.0030\n","Epoch 7/1000\n","1745/1750 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9157\n","Epoch 7: val_acc improved from 0.92050 to 0.92475, saving model to cv_study1.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2473 - acc: 0.9157 - val_loss: 0.2031 - val_acc: 0.9247 - lr: 0.0030\n","Epoch 8/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9183\n","Epoch 8: val_acc did not improve from 0.92475\n","1750/1750 [==============================] - 18s 11ms/step - loss: 0.2397 - acc: 0.9183 - val_loss: 0.2208 - val_acc: 0.9208 - lr: 0.0030\n","Epoch 9/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9222\n","Epoch 9: val_acc did not improve from 0.92475\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2286 - acc: 0.9222 - val_loss: 0.2075 - val_acc: 0.9247 - lr: 0.0030\n","Epoch 10/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9238\n","Epoch 10: val_acc improved from 0.92475 to 0.93175, saving model to cv_study1.h5\n","1750/1750 [==============================] - 22s 13ms/step - loss: 0.2226 - acc: 0.9238 - val_loss: 0.1938 - val_acc: 0.9317 - lr: 0.0030\n","Epoch 11/1000\n","1745/1750 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9269\n","Epoch 11: val_acc improved from 0.93175 to 0.93350, saving model to cv_study1.h5\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.2168 - acc: 0.9269 - val_loss: 0.1873 - val_acc: 0.9335 - lr: 0.0030\n","Epoch 12/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9291\n","Epoch 12: val_acc did not improve from 0.93350\n","1750/1750 [==============================] - 17s 10ms/step - loss: 0.2087 - acc: 0.9291 - val_loss: 0.1994 - val_acc: 0.9298 - lr: 0.0030\n","Epoch 13/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9328\n","Epoch 13: val_acc did not improve from 0.93350\n","1750/1750 [==============================] - 17s 10ms/step - loss: 0.1991 - acc: 0.9328 - val_loss: 0.2009 - val_acc: 0.9312 - lr: 0.0030\n","Epoch 14/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1987 - acc: 0.9336\n","Epoch 14: val_acc did not improve from 0.93350\n","1750/1750 [==============================] - 18s 11ms/step - loss: 0.1987 - acc: 0.9336 - val_loss: 0.1957 - val_acc: 0.9305 - lr: 0.0030\n","Epoch 15/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1909 - acc: 0.9361\n","Epoch 15: val_acc improved from 0.93350 to 0.93425, saving model to cv_study1.h5\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1909 - acc: 0.9361 - val_loss: 0.1835 - val_acc: 0.9342 - lr: 0.0030\n","Epoch 16/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1897 - acc: 0.9364\n","Epoch 16: val_acc improved from 0.93425 to 0.93525, saving model to cv_study1.h5\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1897 - acc: 0.9364 - val_loss: 0.1917 - val_acc: 0.9352 - lr: 0.0030\n","Epoch 17/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9390\n","Epoch 17: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1826 - acc: 0.9390 - val_loss: 0.1912 - val_acc: 0.9325 - lr: 0.0030\n","Epoch 18/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9401\n","Epoch 18: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1816 - acc: 0.9400 - val_loss: 0.1883 - val_acc: 0.9345 - lr: 0.0030\n","Epoch 19/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9418\n","Epoch 19: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1757 - acc: 0.9418 - val_loss: 0.1885 - val_acc: 0.9320 - lr: 0.0030\n","Epoch 20/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9417\n","Epoch 20: val_acc improved from 0.93525 to 0.93825, saving model to cv_study1.h5\n","1750/1750 [==============================] - 17s 10ms/step - loss: 0.1735 - acc: 0.9417 - val_loss: 0.1808 - val_acc: 0.9383 - lr: 0.0030\n","Epoch 21/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9444\n","Epoch 21: val_acc did not improve from 0.93825\n","1750/1750 [==============================] - 18s 11ms/step - loss: 0.1672 - acc: 0.9444 - val_loss: 0.1929 - val_acc: 0.9348 - lr: 0.0030\n","Epoch 22/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9439\n","Epoch 22: val_acc did not improve from 0.93825\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1678 - acc: 0.9440 - val_loss: 0.1915 - val_acc: 0.9358 - lr: 0.0030\n","Epoch 23/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9461\n","Epoch 23: val_acc did not improve from 0.93825\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1609 - acc: 0.9461 - val_loss: 0.1921 - val_acc: 0.9380 - lr: 0.0030\n","Epoch 24/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9450\n","Epoch 24: val_acc did not improve from 0.93825\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1660 - acc: 0.9450 - val_loss: 0.2031 - val_acc: 0.9305 - lr: 0.0030\n","Epoch 25/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9476\n","Epoch 25: val_acc improved from 0.93825 to 0.93850, saving model to cv_study1.h5\n","\n","Epoch 25: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 17s 10ms/step - loss: 0.1560 - acc: 0.9476 - val_loss: 0.1880 - val_acc: 0.9385 - lr: 0.0030\n","Epoch 26/1000\n","1745/1750 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9553\n","Epoch 26: val_acc improved from 0.93850 to 0.94225, saving model to cv_study1.h5\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1363 - acc: 0.9553 - val_loss: 0.1817 - val_acc: 0.9423 - lr: 0.0015\n","Epoch 27/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1296 - acc: 0.9567\n","Epoch 27: val_acc did not improve from 0.94225\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1296 - acc: 0.9567 - val_loss: 0.1849 - val_acc: 0.9388 - lr: 0.0015\n","Epoch 28/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9577\n","Epoch 28: val_acc did not improve from 0.94225\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1264 - acc: 0.9578 - val_loss: 0.1871 - val_acc: 0.9398 - lr: 0.0015\n","Epoch 29/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9590\n","Epoch 29: val_acc improved from 0.94225 to 0.94600, saving model to cv_study1.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1233 - acc: 0.9590 - val_loss: 0.1824 - val_acc: 0.9460 - lr: 0.0015\n","Epoch 30/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9607\n","Epoch 30: val_acc did not improve from 0.94600\n","\n","Epoch 30: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1194 - acc: 0.9607 - val_loss: 0.1815 - val_acc: 0.9435 - lr: 0.0015\n","Epoch 31/1000\n","1745/1750 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9641\n","Epoch 31: val_acc did not improve from 0.94600\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1095 - acc: 0.9641 - val_loss: 0.1831 - val_acc: 0.9442 - lr: 7.5000e-04\n","Epoch 32/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9654\n","Epoch 32: val_acc improved from 0.94600 to 0.94750, saving model to cv_study1.h5\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1061 - acc: 0.9654 - val_loss: 0.1915 - val_acc: 0.9475 - lr: 7.5000e-04\n","Epoch 33/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1037 - acc: 0.9649\n","Epoch 33: val_acc did not improve from 0.94750\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1037 - acc: 0.9649 - val_loss: 0.1908 - val_acc: 0.9457 - lr: 7.5000e-04\n","Epoch 34/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9664\n","Epoch 34: val_acc did not improve from 0.94750\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1029 - acc: 0.9663 - val_loss: 0.1939 - val_acc: 0.9473 - lr: 7.5000e-04\n","Epoch 35/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9678\n","Epoch 35: val_acc did not improve from 0.94750\n","\n","Epoch 35: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.0992 - acc: 0.9678 - val_loss: 0.1908 - val_acc: 0.9448 - lr: 7.5000e-04\n","Epoch 36/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9686\n","Epoch 36: val_acc improved from 0.94750 to 0.94825, saving model to cv_study1.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.0958 - acc: 0.9686 - val_loss: 0.1955 - val_acc: 0.9482 - lr: 3.7500e-04\n","Epoch 37/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.0923 - acc: 0.9695\n","Epoch 37: val_acc did not improve from 0.94825\n","1750/1750 [==============================] - 18s 11ms/step - loss: 0.0923 - acc: 0.9695 - val_loss: 0.1949 - val_acc: 0.9482 - lr: 3.7500e-04\n","Epoch 38/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9701\n","Epoch 38: val_acc did not improve from 0.94825\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.0909 - acc: 0.9701 - val_loss: 0.1907 - val_acc: 0.9467 - lr: 3.7500e-04\n","Epoch 39/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.0874 - acc: 0.9705\n","Epoch 39: val_acc did not improve from 0.94825\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.0874 - acc: 0.9705 - val_loss: 0.1936 - val_acc: 0.9463 - lr: 3.7500e-04\n","Epoch 40/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9711\n","Epoch 40: val_acc did not improve from 0.94825\n","\n","Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.0886 - acc: 0.9711 - val_loss: 0.1958 - val_acc: 0.9475 - lr: 3.7500e-04\n","Epoch 41/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9715\n","Epoch 41: val_acc did not improve from 0.94825\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.0860 - acc: 0.9716 - val_loss: 0.1953 - val_acc: 0.9475 - lr: 1.8750e-04\n","Epoch 41: early stopping\n","1번째 학습 FOLD 학습 완료\n","\n","=========================\n","2번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.8050\n","Epoch 1: val_acc improved from -inf to 0.87825, saving model to cv_study2.h5\n","1750/1750 [==============================] - 22s 11ms/step - loss: 0.5492 - acc: 0.8052 - val_loss: 0.3296 - val_acc: 0.8783 - lr: 0.0030\n","Epoch 2/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8761\n","Epoch 2: val_acc improved from 0.87825 to 0.90450, saving model to cv_study2.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.3574 - acc: 0.8761 - val_loss: 0.2669 - val_acc: 0.9045 - lr: 0.0030\n","Epoch 3/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.3142 - acc: 0.8935\n","Epoch 3: val_acc did not improve from 0.90450\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.3142 - acc: 0.8935 - val_loss: 0.2652 - val_acc: 0.9022 - lr: 0.0030\n","Epoch 4/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2917 - acc: 0.9003\n","Epoch 4: val_acc improved from 0.90450 to 0.92125, saving model to cv_study2.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2917 - acc: 0.9003 - val_loss: 0.2396 - val_acc: 0.9212 - lr: 0.0030\n","Epoch 5/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9078\n","Epoch 5: val_acc did not improve from 0.92125\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2711 - acc: 0.9079 - val_loss: 0.2418 - val_acc: 0.9135 - lr: 0.0030\n","Epoch 6/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9128\n","Epoch 6: val_acc did not improve from 0.92125\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2572 - acc: 0.9128 - val_loss: 0.2250 - val_acc: 0.9202 - lr: 0.0030\n","Epoch 7/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9165\n","Epoch 7: val_acc improved from 0.92125 to 0.92575, saving model to cv_study2.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2464 - acc: 0.9166 - val_loss: 0.2063 - val_acc: 0.9258 - lr: 0.0030\n","Epoch 8/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9191\n","Epoch 8: val_acc did not improve from 0.92575\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2396 - acc: 0.9191 - val_loss: 0.2058 - val_acc: 0.9252 - lr: 0.0030\n","Epoch 9/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9221\n","Epoch 9: val_acc improved from 0.92575 to 0.93225, saving model to cv_study2.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2285 - acc: 0.9221 - val_loss: 0.1962 - val_acc: 0.9323 - lr: 0.0030\n","Epoch 10/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9253\n","Epoch 10: val_acc did not improve from 0.93225\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2203 - acc: 0.9253 - val_loss: 0.2032 - val_acc: 0.9290 - lr: 0.0030\n","Epoch 11/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9286\n","Epoch 11: val_acc did not improve from 0.93225\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2106 - acc: 0.9287 - val_loss: 0.1931 - val_acc: 0.9317 - lr: 0.0030\n","Epoch 12/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2094 - acc: 0.9307\n","Epoch 12: val_acc did not improve from 0.93225\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2094 - acc: 0.9307 - val_loss: 0.1981 - val_acc: 0.9300 - lr: 0.0030\n","Epoch 13/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9329\n","Epoch 13: val_acc did not improve from 0.93225\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2019 - acc: 0.9329 - val_loss: 0.2001 - val_acc: 0.9312 - lr: 0.0030\n","Epoch 14/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9343\n","Epoch 14: val_acc improved from 0.93225 to 0.93700, saving model to cv_study2.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1962 - acc: 0.9343 - val_loss: 0.1893 - val_acc: 0.9370 - lr: 0.0030\n","Epoch 15/1000\n","1745/1750 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9359\n","Epoch 15: val_acc did not improve from 0.93700\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1917 - acc: 0.9359 - val_loss: 0.1994 - val_acc: 0.9350 - lr: 0.0030\n","Epoch 16/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9377\n","Epoch 16: val_acc improved from 0.93700 to 0.93725, saving model to cv_study2.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1855 - acc: 0.9376 - val_loss: 0.1982 - val_acc: 0.9373 - lr: 0.0030\n","Epoch 17/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9400\n","Epoch 17: val_acc did not improve from 0.93725\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1791 - acc: 0.9401 - val_loss: 0.2046 - val_acc: 0.9310 - lr: 0.0030\n","Epoch 18/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9412\n","Epoch 18: val_acc did not improve from 0.93725\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1762 - acc: 0.9412 - val_loss: 0.1994 - val_acc: 0.9352 - lr: 0.0030\n","Epoch 19/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9431\n","Epoch 19: val_acc did not improve from 0.93725\n","\n","Epoch 19: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1743 - acc: 0.9431 - val_loss: 0.2067 - val_acc: 0.9323 - lr: 0.0030\n","Epoch 20/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9504\n","Epoch 20: val_acc improved from 0.93725 to 0.93875, saving model to cv_study2.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1471 - acc: 0.9505 - val_loss: 0.1924 - val_acc: 0.9388 - lr: 0.0015\n","Epoch 21/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9520\n","Epoch 21: val_acc did not improve from 0.93875\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1426 - acc: 0.9520 - val_loss: 0.2029 - val_acc: 0.9375 - lr: 0.0015\n","Epoch 22/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.9534\n","Epoch 22: val_acc improved from 0.93875 to 0.94375, saving model to cv_study2.h5\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1380 - acc: 0.9534 - val_loss: 0.1862 - val_acc: 0.9438 - lr: 0.0015\n","Epoch 23/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9554\n","Epoch 23: val_acc did not improve from 0.94375\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1345 - acc: 0.9554 - val_loss: 0.1905 - val_acc: 0.9400 - lr: 0.0015\n","Epoch 24/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9564\n","Epoch 24: val_acc did not improve from 0.94375\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1305 - acc: 0.9564 - val_loss: 0.1916 - val_acc: 0.9425 - lr: 0.0015\n","Epoch 25/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9571\n","Epoch 25: val_acc did not improve from 0.94375\n","1750/1750 [==============================] - 18s 10ms/step - loss: 0.1290 - acc: 0.9571 - val_loss: 0.1909 - val_acc: 0.9425 - lr: 0.0015\n","Epoch 26/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9582\n","Epoch 26: val_acc did not improve from 0.94375\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1258 - acc: 0.9581 - val_loss: 0.1814 - val_acc: 0.9435 - lr: 0.0015\n","Epoch 27/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9592\n","Epoch 27: val_acc did not improve from 0.94375\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1224 - acc: 0.9591 - val_loss: 0.2006 - val_acc: 0.9398 - lr: 0.0015\n","Epoch 27: early stopping\n","2번째 학습 FOLD 학습 완료\n","\n","=========================\n","3번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.5766 - acc: 0.7942\n","Epoch 1: val_acc improved from -inf to 0.86625, saving model to cv_study3.h5\n","1750/1750 [==============================] - 21s 11ms/step - loss: 0.5766 - acc: 0.7942 - val_loss: 0.3715 - val_acc: 0.8662 - lr: 0.0030\n","Epoch 2/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.3579 - acc: 0.8770\n","Epoch 2: val_acc improved from 0.86625 to 0.87250, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.3579 - acc: 0.8770 - val_loss: 0.3510 - val_acc: 0.8725 - lr: 0.0030\n","Epoch 3/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.8918\n","Epoch 3: val_acc improved from 0.87250 to 0.91050, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.3130 - acc: 0.8919 - val_loss: 0.2646 - val_acc: 0.9105 - lr: 0.0030\n","Epoch 4/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.9015\n","Epoch 4: val_acc did not improve from 0.91050\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2877 - acc: 0.9016 - val_loss: 0.3153 - val_acc: 0.8928 - lr: 0.0030\n","Epoch 5/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9081\n","Epoch 5: val_acc did not improve from 0.91050\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2719 - acc: 0.9081 - val_loss: 0.3137 - val_acc: 0.8885 - lr: 0.0030\n","Epoch 6/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9123\n","Epoch 6: val_acc improved from 0.91050 to 0.92000, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2554 - acc: 0.9123 - val_loss: 0.2438 - val_acc: 0.9200 - lr: 0.0030\n","Epoch 7/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2439 - acc: 0.9180\n","Epoch 7: val_acc did not improve from 0.92000\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2439 - acc: 0.9180 - val_loss: 0.2376 - val_acc: 0.9175 - lr: 0.0030\n","Epoch 8/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9206\n","Epoch 8: val_acc did not improve from 0.92000\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2361 - acc: 0.9206 - val_loss: 0.2536 - val_acc: 0.9107 - lr: 0.0030\n","Epoch 9/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9227\n","Epoch 9: val_acc improved from 0.92000 to 0.92400, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2273 - acc: 0.9227 - val_loss: 0.2326 - val_acc: 0.9240 - lr: 0.0030\n","Epoch 10/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2202 - acc: 0.9256\n","Epoch 10: val_acc improved from 0.92400 to 0.92450, saving model to cv_study3.h5\n","1750/1750 [==============================] - 18s 11ms/step - loss: 0.2202 - acc: 0.9256 - val_loss: 0.2164 - val_acc: 0.9245 - lr: 0.0030\n","Epoch 11/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9284\n","Epoch 11: val_acc improved from 0.92450 to 0.92550, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2091 - acc: 0.9284 - val_loss: 0.2372 - val_acc: 0.9255 - lr: 0.0030\n","Epoch 12/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9311\n","Epoch 12: val_acc improved from 0.92550 to 0.92725, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2051 - acc: 0.9311 - val_loss: 0.2151 - val_acc: 0.9273 - lr: 0.0030\n","Epoch 13/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9319\n","Epoch 13: val_acc did not improve from 0.92725\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2009 - acc: 0.9319 - val_loss: 0.2326 - val_acc: 0.9190 - lr: 0.0030\n","Epoch 14/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9349\n","Epoch 14: val_acc improved from 0.92725 to 0.93050, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1932 - acc: 0.9349 - val_loss: 0.2079 - val_acc: 0.9305 - lr: 0.0030\n","Epoch 15/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1966 - acc: 0.9346\n","Epoch 15: val_acc did not improve from 0.93050\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1966 - acc: 0.9346 - val_loss: 0.2180 - val_acc: 0.9258 - lr: 0.0030\n","Epoch 16/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9377\n","Epoch 16: val_acc did not improve from 0.93050\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1851 - acc: 0.9377 - val_loss: 0.2220 - val_acc: 0.9245 - lr: 0.0030\n","Epoch 17/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1787 - acc: 0.9394\n","Epoch 17: val_acc improved from 0.93050 to 0.93075, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1787 - acc: 0.9394 - val_loss: 0.2038 - val_acc: 0.9308 - lr: 0.0030\n","Epoch 18/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9400\n","Epoch 18: val_acc did not improve from 0.93075\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1776 - acc: 0.9401 - val_loss: 0.2136 - val_acc: 0.9280 - lr: 0.0030\n","Epoch 19/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9421\n","Epoch 19: val_acc improved from 0.93075 to 0.93425, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1713 - acc: 0.9421 - val_loss: 0.2024 - val_acc: 0.9342 - lr: 0.0030\n","Epoch 20/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9437\n","Epoch 20: val_acc did not improve from 0.93425\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1680 - acc: 0.9437 - val_loss: 0.2176 - val_acc: 0.9317 - lr: 0.0030\n","Epoch 21/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9455\n","Epoch 21: val_acc did not improve from 0.93425\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1641 - acc: 0.9455 - val_loss: 0.2046 - val_acc: 0.9335 - lr: 0.0030\n","Epoch 22/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9449\n","Epoch 22: val_acc did not improve from 0.93425\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1633 - acc: 0.9449 - val_loss: 0.2119 - val_acc: 0.9315 - lr: 0.0030\n","Epoch 23/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1611 - acc: 0.9462\n","Epoch 23: val_acc improved from 0.93425 to 0.93575, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1611 - acc: 0.9462 - val_loss: 0.2005 - val_acc: 0.9358 - lr: 0.0030\n","Epoch 24/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9475\n","Epoch 24: val_acc did not improve from 0.93575\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1581 - acc: 0.9475 - val_loss: 0.2244 - val_acc: 0.9275 - lr: 0.0030\n","Epoch 25/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1546 - acc: 0.9484\n","Epoch 25: val_acc improved from 0.93575 to 0.93775, saving model to cv_study3.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1546 - acc: 0.9484 - val_loss: 0.1966 - val_acc: 0.9377 - lr: 0.0030\n","Epoch 26/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9501\n","Epoch 26: val_acc did not improve from 0.93775\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1525 - acc: 0.9502 - val_loss: 0.2219 - val_acc: 0.9290 - lr: 0.0030\n","Epoch 27/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1461 - acc: 0.9510\n","Epoch 27: val_acc did not improve from 0.93775\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1461 - acc: 0.9510 - val_loss: 0.2264 - val_acc: 0.9312 - lr: 0.0030\n","Epoch 28/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9517\n","Epoch 28: val_acc did not improve from 0.93775\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1465 - acc: 0.9516 - val_loss: 0.2242 - val_acc: 0.9320 - lr: 0.0030\n","Epoch 29/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1461 - acc: 0.9517\n","Epoch 29: val_acc did not improve from 0.93775\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1461 - acc: 0.9517 - val_loss: 0.2160 - val_acc: 0.9355 - lr: 0.0030\n","Epoch 30/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1413 - acc: 0.9528\n","Epoch 30: val_acc did not improve from 0.93775\n","\n","Epoch 30: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1413 - acc: 0.9528 - val_loss: 0.2162 - val_acc: 0.9325 - lr: 0.0030\n","Epoch 30: early stopping\n","3번째 학습 FOLD 학습 완료\n","\n","=========================\n","4번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.5639 - acc: 0.7990\n","Epoch 1: val_acc improved from -inf to 0.87825, saving model to cv_study4.h5\n","1750/1750 [==============================] - 21s 11ms/step - loss: 0.5642 - acc: 0.7989 - val_loss: 0.3232 - val_acc: 0.8783 - lr: 0.0030\n","Epoch 2/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.3602 - acc: 0.8764\n","Epoch 2: val_acc improved from 0.87825 to 0.88000, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.3603 - acc: 0.8764 - val_loss: 0.3205 - val_acc: 0.8800 - lr: 0.0030\n","Epoch 3/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8926\n","Epoch 3: val_acc improved from 0.88000 to 0.90525, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.3144 - acc: 0.8926 - val_loss: 0.2625 - val_acc: 0.9053 - lr: 0.0030\n","Epoch 4/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2905 - acc: 0.9004\n","Epoch 4: val_acc improved from 0.90525 to 0.91825, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2905 - acc: 0.9004 - val_loss: 0.2285 - val_acc: 0.9183 - lr: 0.0030\n","Epoch 5/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9077\n","Epoch 5: val_acc did not improve from 0.91825\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2720 - acc: 0.9077 - val_loss: 0.2575 - val_acc: 0.9078 - lr: 0.0030\n","Epoch 6/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9131\n","Epoch 6: val_acc did not improve from 0.91825\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2557 - acc: 0.9131 - val_loss: 0.2363 - val_acc: 0.9180 - lr: 0.0030\n","Epoch 7/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9177\n","Epoch 7: val_acc improved from 0.91825 to 0.92125, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2460 - acc: 0.9177 - val_loss: 0.2183 - val_acc: 0.9212 - lr: 0.0030\n","Epoch 8/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2368 - acc: 0.9195\n","Epoch 8: val_acc improved from 0.92125 to 0.93400, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2368 - acc: 0.9195 - val_loss: 0.1974 - val_acc: 0.9340 - lr: 0.0030\n","Epoch 9/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9240\n","Epoch 9: val_acc did not improve from 0.93400\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2266 - acc: 0.9240 - val_loss: 0.1968 - val_acc: 0.9295 - lr: 0.0030\n","Epoch 10/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9243\n","Epoch 10: val_acc did not improve from 0.93400\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2220 - acc: 0.9243 - val_loss: 0.1950 - val_acc: 0.9310 - lr: 0.0030\n","Epoch 11/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9281\n","Epoch 11: val_acc did not improve from 0.93400\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2125 - acc: 0.9281 - val_loss: 0.1881 - val_acc: 0.9310 - lr: 0.0030\n","Epoch 12/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2052 - acc: 0.9312\n","Epoch 12: val_acc improved from 0.93400 to 0.93575, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2052 - acc: 0.9312 - val_loss: 0.1892 - val_acc: 0.9358 - lr: 0.0030\n","Epoch 13/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9323\n","Epoch 13: val_acc did not improve from 0.93575\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2009 - acc: 0.9323 - val_loss: 0.1834 - val_acc: 0.9358 - lr: 0.0030\n","Epoch 14/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9342\n","Epoch 14: val_acc did not improve from 0.93575\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1973 - acc: 0.9342 - val_loss: 0.1975 - val_acc: 0.9333 - lr: 0.0030\n","Epoch 15/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1920 - acc: 0.9357\n","Epoch 15: val_acc did not improve from 0.93575\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1920 - acc: 0.9357 - val_loss: 0.1974 - val_acc: 0.9352 - lr: 0.0030\n","Epoch 16/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1859 - acc: 0.9384\n","Epoch 16: val_acc improved from 0.93575 to 0.93750, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1859 - acc: 0.9384 - val_loss: 0.1889 - val_acc: 0.9375 - lr: 0.0030\n","Epoch 17/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9390\n","Epoch 17: val_acc did not improve from 0.93750\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1819 - acc: 0.9390 - val_loss: 0.1818 - val_acc: 0.9350 - lr: 0.0030\n","Epoch 18/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1793 - acc: 0.9402\n","Epoch 18: val_acc improved from 0.93750 to 0.93850, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1793 - acc: 0.9402 - val_loss: 0.2013 - val_acc: 0.9385 - lr: 0.0030\n","Epoch 19/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1742 - acc: 0.9427\n","Epoch 19: val_acc improved from 0.93850 to 0.93975, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1742 - acc: 0.9427 - val_loss: 0.1913 - val_acc: 0.9398 - lr: 0.0030\n","Epoch 20/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9445\n","Epoch 20: val_acc did not improve from 0.93975\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1689 - acc: 0.9446 - val_loss: 0.1916 - val_acc: 0.9395 - lr: 0.0030\n","Epoch 21/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9448\n","Epoch 21: val_acc did not improve from 0.93975\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1670 - acc: 0.9449 - val_loss: 0.2070 - val_acc: 0.9345 - lr: 0.0030\n","Epoch 22/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9459\n","Epoch 22: val_acc improved from 0.93975 to 0.94200, saving model to cv_study4.h5\n","\n","Epoch 22: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1629 - acc: 0.9460 - val_loss: 0.1916 - val_acc: 0.9420 - lr: 0.0030\n","Epoch 23/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9536\n","Epoch 23: val_acc improved from 0.94200 to 0.94475, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1400 - acc: 0.9535 - val_loss: 0.1810 - val_acc: 0.9448 - lr: 0.0015\n","Epoch 24/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9557\n","Epoch 24: val_acc improved from 0.94475 to 0.94600, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1327 - acc: 0.9558 - val_loss: 0.1837 - val_acc: 0.9460 - lr: 0.0015\n","Epoch 25/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9585\n","Epoch 25: val_acc did not improve from 0.94600\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1273 - acc: 0.9585 - val_loss: 0.1747 - val_acc: 0.9450 - lr: 0.0015\n","Epoch 26/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9576\n","Epoch 26: val_acc improved from 0.94600 to 0.94625, saving model to cv_study4.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1259 - acc: 0.9576 - val_loss: 0.1823 - val_acc: 0.9463 - lr: 0.0015\n","Epoch 27/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9591\n","Epoch 27: val_acc did not improve from 0.94625\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1246 - acc: 0.9591 - val_loss: 0.1811 - val_acc: 0.9452 - lr: 0.0015\n","Epoch 28/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9606\n","Epoch 28: val_acc did not improve from 0.94625\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1195 - acc: 0.9605 - val_loss: 0.1875 - val_acc: 0.9442 - lr: 0.0015\n","Epoch 29/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9607\n","Epoch 29: val_acc did not improve from 0.94625\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1192 - acc: 0.9607 - val_loss: 0.1960 - val_acc: 0.9438 - lr: 0.0015\n","Epoch 30/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1159 - acc: 0.9629\n","Epoch 30: val_acc did not improve from 0.94625\n","\n","Epoch 30: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1159 - acc: 0.9629 - val_loss: 0.1989 - val_acc: 0.9413 - lr: 0.0015\n","Epoch 31/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9654\n","Epoch 31: val_acc did not improve from 0.94625\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1033 - acc: 0.9653 - val_loss: 0.1924 - val_acc: 0.9457 - lr: 7.5000e-04\n","Epoch 31: early stopping\n","4번째 학습 FOLD 학습 완료\n","\n","=========================\n","5번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.5650 - acc: 0.7988\n","Epoch 1: val_acc improved from -inf to 0.88050, saving model to cv_study5.h5\n","1750/1750 [==============================] - 22s 11ms/step - loss: 0.5647 - acc: 0.7989 - val_loss: 0.3217 - val_acc: 0.8805 - lr: 0.0030\n","Epoch 2/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8751\n","Epoch 2: val_acc improved from 0.88050 to 0.88375, saving model to cv_study5.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3614 - acc: 0.8752 - val_loss: 0.3084 - val_acc: 0.8838 - lr: 0.0030\n","Epoch 3/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3170 - acc: 0.8899\n","Epoch 3: val_acc improved from 0.88375 to 0.89975, saving model to cv_study5.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3171 - acc: 0.8899 - val_loss: 0.2670 - val_acc: 0.8997 - lr: 0.0030\n","Epoch 4/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.8981\n","Epoch 4: val_acc improved from 0.89975 to 0.91800, saving model to cv_study5.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2947 - acc: 0.8981 - val_loss: 0.2350 - val_acc: 0.9180 - lr: 0.0030\n","Epoch 5/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9077\n","Epoch 5: val_acc did not improve from 0.91800\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2734 - acc: 0.9078 - val_loss: 0.3535 - val_acc: 0.8870 - lr: 0.0030\n","Epoch 6/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9111\n","Epoch 6: val_acc improved from 0.91800 to 0.92375, saving model to cv_study5.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2587 - acc: 0.9111 - val_loss: 0.2080 - val_acc: 0.9237 - lr: 0.0030\n","Epoch 7/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9159\n","Epoch 7: val_acc improved from 0.92375 to 0.93075, saving model to cv_study5.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2500 - acc: 0.9159 - val_loss: 0.1947 - val_acc: 0.9308 - lr: 0.0030\n","Epoch 8/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9186\n","Epoch 8: val_acc improved from 0.93075 to 0.93100, saving model to cv_study5.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2375 - acc: 0.9187 - val_loss: 0.1880 - val_acc: 0.9310 - lr: 0.0030\n","Epoch 9/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9221\n","Epoch 9: val_acc did not improve from 0.93100\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2295 - acc: 0.9221 - val_loss: 0.1960 - val_acc: 0.9305 - lr: 0.0030\n","Epoch 10/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9251\n","Epoch 10: val_acc did not improve from 0.93100\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2224 - acc: 0.9252 - val_loss: 0.2093 - val_acc: 0.9252 - lr: 0.0030\n","Epoch 11/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2133 - acc: 0.9283\n","Epoch 11: val_acc did not improve from 0.93100\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2133 - acc: 0.9283 - val_loss: 0.1927 - val_acc: 0.9308 - lr: 0.0030\n","Epoch 12/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2058 - acc: 0.9315\n","Epoch 12: val_acc did not improve from 0.93100\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2058 - acc: 0.9315 - val_loss: 0.1981 - val_acc: 0.9287 - lr: 0.0030\n","Epoch 13/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9327\n","Epoch 13: val_acc did not improve from 0.93100\n","\n","Epoch 13: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2002 - acc: 0.9327 - val_loss: 0.2049 - val_acc: 0.9302 - lr: 0.0030\n","Epoch 13: early stopping\n","5번째 학습 FOLD 학습 완료\n","\n","=========================\n","6번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.8034\n","Epoch 1: val_acc improved from -inf to 0.89000, saving model to cv_study6.h5\n","1750/1750 [==============================] - 22s 11ms/step - loss: 0.5576 - acc: 0.8035 - val_loss: 0.3254 - val_acc: 0.8900 - lr: 0.0030\n","Epoch 2/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8761\n","Epoch 2: val_acc improved from 0.89000 to 0.89925, saving model to cv_study6.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3590 - acc: 0.8761 - val_loss: 0.2790 - val_acc: 0.8992 - lr: 0.0030\n","Epoch 3/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8920\n","Epoch 3: val_acc did not improve from 0.89925\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.3157 - acc: 0.8920 - val_loss: 0.3025 - val_acc: 0.8947 - lr: 0.0030\n","Epoch 4/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9014\n","Epoch 4: val_acc improved from 0.89925 to 0.91375, saving model to cv_study6.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2884 - acc: 0.9014 - val_loss: 0.2438 - val_acc: 0.9137 - lr: 0.0030\n","Epoch 5/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9074\n","Epoch 5: val_acc improved from 0.91375 to 0.92225, saving model to cv_study6.h5\n","1750/1750 [==============================] - 22s 13ms/step - loss: 0.2735 - acc: 0.9074 - val_loss: 0.2345 - val_acc: 0.9222 - lr: 0.0030\n","Epoch 6/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9130\n","Epoch 6: val_acc did not improve from 0.92225\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2545 - acc: 0.9131 - val_loss: 0.2244 - val_acc: 0.9197 - lr: 0.0030\n","Epoch 7/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9168\n","Epoch 7: val_acc improved from 0.92225 to 0.92625, saving model to cv_study6.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2433 - acc: 0.9169 - val_loss: 0.2088 - val_acc: 0.9262 - lr: 0.0030\n","Epoch 8/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2373 - acc: 0.9182\n","Epoch 8: val_acc improved from 0.92625 to 0.92900, saving model to cv_study6.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2373 - acc: 0.9182 - val_loss: 0.2152 - val_acc: 0.9290 - lr: 0.0030\n","Epoch 9/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9233\n","Epoch 9: val_acc did not improve from 0.92900\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2282 - acc: 0.9234 - val_loss: 0.2342 - val_acc: 0.9208 - lr: 0.0030\n","Epoch 10/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9263\n","Epoch 10: val_acc did not improve from 0.92900\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2188 - acc: 0.9263 - val_loss: 0.2012 - val_acc: 0.9287 - lr: 0.0030\n","Epoch 11/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2093 - acc: 0.9301\n","Epoch 11: val_acc did not improve from 0.92900\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2092 - acc: 0.9302 - val_loss: 0.2149 - val_acc: 0.9262 - lr: 0.0030\n","Epoch 12/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9307\n","Epoch 12: val_acc improved from 0.92900 to 0.93200, saving model to cv_study6.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2060 - acc: 0.9307 - val_loss: 0.2119 - val_acc: 0.9320 - lr: 0.0030\n","Epoch 13/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9333\n","Epoch 13: val_acc did not improve from 0.93200\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2003 - acc: 0.9333 - val_loss: 0.2025 - val_acc: 0.9312 - lr: 0.0030\n","Epoch 14/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9337\n","Epoch 14: val_acc improved from 0.93200 to 0.93325, saving model to cv_study6.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1950 - acc: 0.9337 - val_loss: 0.1978 - val_acc: 0.9333 - lr: 0.0030\n","Epoch 15/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9368\n","Epoch 15: val_acc did not improve from 0.93325\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1906 - acc: 0.9368 - val_loss: 0.2390 - val_acc: 0.9227 - lr: 0.0030\n","Epoch 16/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1836 - acc: 0.9386\n","Epoch 16: val_acc did not improve from 0.93325\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1836 - acc: 0.9386 - val_loss: 0.1946 - val_acc: 0.9312 - lr: 0.0030\n","Epoch 17/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9400\n","Epoch 17: val_acc improved from 0.93325 to 0.93400, saving model to cv_study6.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1803 - acc: 0.9399 - val_loss: 0.1894 - val_acc: 0.9340 - lr: 0.0030\n","Epoch 18/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9402\n","Epoch 18: val_acc did not improve from 0.93400\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1780 - acc: 0.9402 - val_loss: 0.2085 - val_acc: 0.9305 - lr: 0.0030\n","Epoch 19/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9439\n","Epoch 19: val_acc did not improve from 0.93400\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1675 - acc: 0.9439 - val_loss: 0.2108 - val_acc: 0.9327 - lr: 0.0030\n","Epoch 20/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1697 - acc: 0.9436\n","Epoch 20: val_acc improved from 0.93400 to 0.93800, saving model to cv_study6.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1697 - acc: 0.9436 - val_loss: 0.1774 - val_acc: 0.9380 - lr: 0.0030\n","Epoch 21/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9451\n","Epoch 21: val_acc did not improve from 0.93800\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1660 - acc: 0.9450 - val_loss: 0.2073 - val_acc: 0.9330 - lr: 0.0030\n","Epoch 22/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9457\n","Epoch 22: val_acc improved from 0.93800 to 0.93875, saving model to cv_study6.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1640 - acc: 0.9457 - val_loss: 0.1921 - val_acc: 0.9388 - lr: 0.0030\n","Epoch 23/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9471\n","Epoch 23: val_acc improved from 0.93875 to 0.94175, saving model to cv_study6.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1593 - acc: 0.9471 - val_loss: 0.1839 - val_acc: 0.9417 - lr: 0.0030\n","Epoch 24/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1594 - acc: 0.9474\n","Epoch 24: val_acc did not improve from 0.94175\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1594 - acc: 0.9474 - val_loss: 0.1936 - val_acc: 0.9370 - lr: 0.0030\n","Epoch 25/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9498\n","Epoch 25: val_acc did not improve from 0.94175\n","\n","Epoch 25: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1521 - acc: 0.9498 - val_loss: 0.1977 - val_acc: 0.9365 - lr: 0.0030\n","Epoch 26/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9551\n","Epoch 26: val_acc did not improve from 0.94175\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1321 - acc: 0.9551 - val_loss: 0.1908 - val_acc: 0.9415 - lr: 0.0015\n","Epoch 27/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1241 - acc: 0.9594\n","Epoch 27: val_acc improved from 0.94175 to 0.94425, saving model to cv_study6.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1241 - acc: 0.9594 - val_loss: 0.1872 - val_acc: 0.9442 - lr: 0.0015\n","Epoch 28/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9599\n","Epoch 28: val_acc did not improve from 0.94425\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1209 - acc: 0.9598 - val_loss: 0.1786 - val_acc: 0.9433 - lr: 0.0015\n","Epoch 29/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9619\n","Epoch 29: val_acc improved from 0.94425 to 0.94575, saving model to cv_study6.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1159 - acc: 0.9619 - val_loss: 0.1845 - val_acc: 0.9457 - lr: 0.0015\n","Epoch 30/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1149 - acc: 0.9624\n","Epoch 30: val_acc did not improve from 0.94575\n","\n","Epoch 30: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1149 - acc: 0.9624 - val_loss: 0.1923 - val_acc: 0.9448 - lr: 0.0015\n","Epoch 31/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9657\n","Epoch 31: val_acc did not improve from 0.94575\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1024 - acc: 0.9657 - val_loss: 0.1984 - val_acc: 0.9425 - lr: 7.5000e-04\n","Epoch 32/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9674\n","Epoch 32: val_acc did not improve from 0.94575\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.0995 - acc: 0.9674 - val_loss: 0.1918 - val_acc: 0.9450 - lr: 7.5000e-04\n","Epoch 33/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9674\n","Epoch 33: val_acc did not improve from 0.94575\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.0999 - acc: 0.9674 - val_loss: 0.2003 - val_acc: 0.9425 - lr: 7.5000e-04\n","Epoch 34/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9691\n","Epoch 34: val_acc did not improve from 0.94575\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.0944 - acc: 0.9692 - val_loss: 0.2002 - val_acc: 0.9430 - lr: 7.5000e-04\n","Epoch 34: early stopping\n","6번째 학습 FOLD 학습 완료\n","\n","=========================\n","7번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.5627 - acc: 0.8021\n","Epoch 1: val_acc improved from -inf to 0.85325, saving model to cv_study7.h5\n","1750/1750 [==============================] - 22s 11ms/step - loss: 0.5625 - acc: 0.8022 - val_loss: 0.4123 - val_acc: 0.8533 - lr: 0.0030\n","Epoch 2/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8752\n","Epoch 2: val_acc improved from 0.85325 to 0.89925, saving model to cv_study7.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3616 - acc: 0.8752 - val_loss: 0.2872 - val_acc: 0.8992 - lr: 0.0030\n","Epoch 3/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.8913\n","Epoch 3: val_acc improved from 0.89925 to 0.90650, saving model to cv_study7.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3209 - acc: 0.8912 - val_loss: 0.2424 - val_acc: 0.9065 - lr: 0.0030\n","Epoch 4/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2924 - acc: 0.8991\n","Epoch 4: val_acc improved from 0.90650 to 0.92275, saving model to cv_study7.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2924 - acc: 0.8991 - val_loss: 0.2225 - val_acc: 0.9227 - lr: 0.0030\n","Epoch 5/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9048\n","Epoch 5: val_acc improved from 0.92275 to 0.92650, saving model to cv_study7.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2777 - acc: 0.9049 - val_loss: 0.2098 - val_acc: 0.9265 - lr: 0.0030\n","Epoch 6/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9098\n","Epoch 6: val_acc did not improve from 0.92650\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2618 - acc: 0.9098 - val_loss: 0.2388 - val_acc: 0.9143 - lr: 0.0030\n","Epoch 7/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2520 - acc: 0.9134\n","Epoch 7: val_acc did not improve from 0.92650\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2520 - acc: 0.9134 - val_loss: 0.2901 - val_acc: 0.9028 - lr: 0.0030\n","Epoch 8/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9173\n","Epoch 8: val_acc did not improve from 0.92650\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2433 - acc: 0.9172 - val_loss: 0.2315 - val_acc: 0.9193 - lr: 0.0030\n","Epoch 9/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2326 - acc: 0.9199\n","Epoch 9: val_acc did not improve from 0.92650\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2326 - acc: 0.9199 - val_loss: 0.2108 - val_acc: 0.9227 - lr: 0.0030\n","Epoch 10/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9228\n","Epoch 10: val_acc improved from 0.92650 to 0.93525, saving model to cv_study7.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2270 - acc: 0.9228 - val_loss: 0.1844 - val_acc: 0.9352 - lr: 0.0030\n","Epoch 11/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9257\n","Epoch 11: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2205 - acc: 0.9257 - val_loss: 0.1744 - val_acc: 0.9345 - lr: 0.0030\n","Epoch 12/1000\n","1745/1750 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9277\n","Epoch 12: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2119 - acc: 0.9276 - val_loss: 0.1943 - val_acc: 0.9287 - lr: 0.0030\n","Epoch 13/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9307\n","Epoch 13: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2080 - acc: 0.9308 - val_loss: 0.2165 - val_acc: 0.9255 - lr: 0.0030\n","Epoch 14/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9301\n","Epoch 14: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2042 - acc: 0.9300 - val_loss: 0.1996 - val_acc: 0.9277 - lr: 0.0030\n","Epoch 15/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1957 - acc: 0.9339\n","Epoch 15: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1957 - acc: 0.9339 - val_loss: 0.1904 - val_acc: 0.9330 - lr: 0.0030\n","Epoch 15: early stopping\n","7번째 학습 FOLD 학습 완료\n","\n","=========================\n","8번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.7965\n","Epoch 1: val_acc improved from -inf to 0.86225, saving model to cv_study8.h5\n","1750/1750 [==============================] - 22s 12ms/step - loss: 0.5721 - acc: 0.7966 - val_loss: 0.3568 - val_acc: 0.8622 - lr: 0.0030\n","Epoch 2/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3679 - acc: 0.8721\n","Epoch 2: val_acc improved from 0.86225 to 0.90025, saving model to cv_study8.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.3677 - acc: 0.8721 - val_loss: 0.2825 - val_acc: 0.9003 - lr: 0.0030\n","Epoch 3/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.8903\n","Epoch 3: val_acc improved from 0.90025 to 0.91225, saving model to cv_study8.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3195 - acc: 0.8903 - val_loss: 0.2434 - val_acc: 0.9122 - lr: 0.0030\n","Epoch 4/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2918 - acc: 0.8994\n","Epoch 4: val_acc improved from 0.91225 to 0.91575, saving model to cv_study8.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2918 - acc: 0.8994 - val_loss: 0.2444 - val_acc: 0.9158 - lr: 0.0030\n","Epoch 5/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9066\n","Epoch 5: val_acc improved from 0.91575 to 0.91675, saving model to cv_study8.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2714 - acc: 0.9066 - val_loss: 0.2410 - val_acc: 0.9168 - lr: 0.0030\n","Epoch 6/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2573 - acc: 0.9136\n","Epoch 6: val_acc did not improve from 0.91675\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2573 - acc: 0.9136 - val_loss: 0.2528 - val_acc: 0.9162 - lr: 0.0030\n","Epoch 7/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9163\n","Epoch 7: val_acc improved from 0.91675 to 0.92300, saving model to cv_study8.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2488 - acc: 0.9162 - val_loss: 0.2176 - val_acc: 0.9230 - lr: 0.0030\n","Epoch 8/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9193\n","Epoch 8: val_acc improved from 0.92300 to 0.92725, saving model to cv_study8.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2374 - acc: 0.9193 - val_loss: 0.2030 - val_acc: 0.9273 - lr: 0.0030\n","Epoch 9/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9224\n","Epoch 9: val_acc did not improve from 0.92725\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2315 - acc: 0.9224 - val_loss: 0.2201 - val_acc: 0.9200 - lr: 0.0030\n","Epoch 10/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9251\n","Epoch 10: val_acc did not improve from 0.92725\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2217 - acc: 0.9251 - val_loss: 0.2078 - val_acc: 0.9273 - lr: 0.0030\n","Epoch 11/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2153 - acc: 0.9276\n","Epoch 11: val_acc improved from 0.92725 to 0.92800, saving model to cv_study8.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2153 - acc: 0.9276 - val_loss: 0.2184 - val_acc: 0.9280 - lr: 0.0030\n","Epoch 12/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9304\n","Epoch 12: val_acc did not improve from 0.92800\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2065 - acc: 0.9304 - val_loss: 0.2244 - val_acc: 0.9268 - lr: 0.0030\n","Epoch 13/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9312\n","Epoch 13: val_acc improved from 0.92800 to 0.93250, saving model to cv_study8.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2034 - acc: 0.9312 - val_loss: 0.2004 - val_acc: 0.9325 - lr: 0.0030\n","Epoch 14/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9335\n","Epoch 14: val_acc improved from 0.93250 to 0.93525, saving model to cv_study8.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1967 - acc: 0.9334 - val_loss: 0.1964 - val_acc: 0.9352 - lr: 0.0030\n","Epoch 15/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9343\n","Epoch 15: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1929 - acc: 0.9343 - val_loss: 0.1981 - val_acc: 0.9348 - lr: 0.0030\n","Epoch 16/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9365\n","Epoch 16: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1881 - acc: 0.9365 - val_loss: 0.2100 - val_acc: 0.9330 - lr: 0.0030\n","Epoch 17/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9382\n","Epoch 17: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1869 - acc: 0.9383 - val_loss: 0.1942 - val_acc: 0.9350 - lr: 0.0030\n","Epoch 18/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1760 - acc: 0.9405\n","Epoch 18: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1760 - acc: 0.9405 - val_loss: 0.1987 - val_acc: 0.9327 - lr: 0.0030\n","Epoch 19/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1746 - acc: 0.9410\n","Epoch 19: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1746 - acc: 0.9410 - val_loss: 0.2065 - val_acc: 0.9327 - lr: 0.0030\n","Epoch 19: early stopping\n","8번째 학습 FOLD 학습 완료\n","\n","=========================\n","9번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.8033\n","Epoch 1: val_acc improved from -inf to 0.88200, saving model to cv_study9.h5\n","1750/1750 [==============================] - 22s 11ms/step - loss: 0.5518 - acc: 0.8034 - val_loss: 0.3094 - val_acc: 0.8820 - lr: 0.0030\n","Epoch 2/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.3539 - acc: 0.8776\n","Epoch 2: val_acc improved from 0.88200 to 0.91575, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3539 - acc: 0.8776 - val_loss: 0.2383 - val_acc: 0.9158 - lr: 0.0030\n","Epoch 3/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.8931\n","Epoch 3: val_acc did not improve from 0.91575\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3095 - acc: 0.8931 - val_loss: 0.2310 - val_acc: 0.9150 - lr: 0.0030\n","Epoch 4/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9013\n","Epoch 4: val_acc improved from 0.91575 to 0.92075, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2861 - acc: 0.9013 - val_loss: 0.2209 - val_acc: 0.9208 - lr: 0.0030\n","Epoch 5/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9077\n","Epoch 5: val_acc improved from 0.92075 to 0.92725, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2686 - acc: 0.9078 - val_loss: 0.2099 - val_acc: 0.9273 - lr: 0.0030\n","Epoch 6/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9135\n","Epoch 6: val_acc improved from 0.92725 to 0.92975, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2550 - acc: 0.9135 - val_loss: 0.2052 - val_acc: 0.9298 - lr: 0.0030\n","Epoch 7/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2420 - acc: 0.9181\n","Epoch 7: val_acc did not improve from 0.92975\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2420 - acc: 0.9181 - val_loss: 0.2520 - val_acc: 0.9145 - lr: 0.0030\n","Epoch 8/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9215\n","Epoch 8: val_acc improved from 0.92975 to 0.93125, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2277 - acc: 0.9215 - val_loss: 0.2068 - val_acc: 0.9312 - lr: 0.0030\n","Epoch 9/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9239\n","Epoch 9: val_acc did not improve from 0.93125\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2250 - acc: 0.9240 - val_loss: 0.2074 - val_acc: 0.9260 - lr: 0.0030\n","Epoch 10/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9261\n","Epoch 10: val_acc did not improve from 0.93125\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2210 - acc: 0.9262 - val_loss: 0.2274 - val_acc: 0.9205 - lr: 0.0030\n","Epoch 11/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9290\n","Epoch 11: val_acc improved from 0.93125 to 0.93700, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2128 - acc: 0.9290 - val_loss: 0.1841 - val_acc: 0.9370 - lr: 0.0030\n","Epoch 12/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9306\n","Epoch 12: val_acc did not improve from 0.93700\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2037 - acc: 0.9307 - val_loss: 0.1917 - val_acc: 0.9348 - lr: 0.0030\n","Epoch 13/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1981 - acc: 0.9329\n","Epoch 13: val_acc did not improve from 0.93700\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1981 - acc: 0.9329 - val_loss: 0.1975 - val_acc: 0.9320 - lr: 0.0030\n","Epoch 14/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1941 - acc: 0.9360\n","Epoch 14: val_acc did not improve from 0.93700\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1941 - acc: 0.9360 - val_loss: 0.2018 - val_acc: 0.9305 - lr: 0.0030\n","Epoch 15/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9352\n","Epoch 15: val_acc did not improve from 0.93700\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1905 - acc: 0.9353 - val_loss: 0.1827 - val_acc: 0.9327 - lr: 0.0030\n","Epoch 16/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9386\n","Epoch 16: val_acc improved from 0.93700 to 0.93825, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1813 - acc: 0.9386 - val_loss: 0.1805 - val_acc: 0.9383 - lr: 0.0030\n","Epoch 17/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9405\n","Epoch 17: val_acc did not improve from 0.93825\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1775 - acc: 0.9405 - val_loss: 0.1876 - val_acc: 0.9370 - lr: 0.0030\n","Epoch 18/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9408\n","Epoch 18: val_acc improved from 0.93825 to 0.93850, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1782 - acc: 0.9408 - val_loss: 0.1806 - val_acc: 0.9385 - lr: 0.0030\n","Epoch 19/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9424\n","Epoch 19: val_acc did not improve from 0.93850\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1724 - acc: 0.9424 - val_loss: 0.2039 - val_acc: 0.9355 - lr: 0.0030\n","Epoch 20/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9434\n","Epoch 20: val_acc did not improve from 0.93850\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1674 - acc: 0.9433 - val_loss: 0.1917 - val_acc: 0.9348 - lr: 0.0030\n","Epoch 21/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9449\n","Epoch 21: val_acc did not improve from 0.93850\n","\n","Epoch 21: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1650 - acc: 0.9449 - val_loss: 0.1941 - val_acc: 0.9337 - lr: 0.0030\n","Epoch 22/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9533\n","Epoch 22: val_acc improved from 0.93850 to 0.94300, saving model to cv_study9.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1397 - acc: 0.9534 - val_loss: 0.1926 - val_acc: 0.9430 - lr: 0.0015\n","Epoch 23/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9548\n","Epoch 23: val_acc improved from 0.94300 to 0.94425, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1336 - acc: 0.9549 - val_loss: 0.1918 - val_acc: 0.9442 - lr: 0.0015\n","Epoch 24/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9557\n","Epoch 24: val_acc did not improve from 0.94425\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1321 - acc: 0.9557 - val_loss: 0.1859 - val_acc: 0.9425 - lr: 0.0015\n","Epoch 25/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9597\n","Epoch 25: val_acc improved from 0.94425 to 0.94525, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1268 - acc: 0.9597 - val_loss: 0.1842 - val_acc: 0.9452 - lr: 0.0015\n","Epoch 26/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9597\n","Epoch 26: val_acc did not improve from 0.94525\n","\n","Epoch 26: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1248 - acc: 0.9597 - val_loss: 0.1884 - val_acc: 0.9408 - lr: 0.0015\n","Epoch 27/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1133 - acc: 0.9627\n","Epoch 27: val_acc did not improve from 0.94525\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1133 - acc: 0.9627 - val_loss: 0.1883 - val_acc: 0.9433 - lr: 7.5000e-04\n","Epoch 28/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9637\n","Epoch 28: val_acc improved from 0.94525 to 0.94550, saving model to cv_study9.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1071 - acc: 0.9637 - val_loss: 0.1911 - val_acc: 0.9455 - lr: 7.5000e-04\n","Epoch 29/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1070 - acc: 0.9649\n","Epoch 29: val_acc did not improve from 0.94550\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1070 - acc: 0.9649 - val_loss: 0.1940 - val_acc: 0.9408 - lr: 7.5000e-04\n","Epoch 30/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9661\n","Epoch 30: val_acc did not improve from 0.94550\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1031 - acc: 0.9661 - val_loss: 0.2049 - val_acc: 0.9410 - lr: 7.5000e-04\n","Epoch 31/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9669\n","Epoch 31: val_acc did not improve from 0.94550\n","\n","Epoch 31: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1006 - acc: 0.9669 - val_loss: 0.2053 - val_acc: 0.9440 - lr: 7.5000e-04\n","Epoch 32/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9684\n","Epoch 32: val_acc did not improve from 0.94550\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.0961 - acc: 0.9685 - val_loss: 0.2102 - val_acc: 0.9427 - lr: 3.7500e-04\n","Epoch 33/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9696\n","Epoch 33: val_acc did not improve from 0.94550\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.0932 - acc: 0.9696 - val_loss: 0.2117 - val_acc: 0.9440 - lr: 3.7500e-04\n","Epoch 33: early stopping\n","9번째 학습 FOLD 학습 완료\n","\n","=========================\n","10번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.8027\n","Epoch 1: val_acc improved from -inf to 0.88250, saving model to cv_study10.h5\n","1750/1750 [==============================] - 22s 12ms/step - loss: 0.5558 - acc: 0.8027 - val_loss: 0.3254 - val_acc: 0.8825 - lr: 0.0030\n","Epoch 2/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8733\n","Epoch 2: val_acc improved from 0.88250 to 0.90500, saving model to cv_study10.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3655 - acc: 0.8733 - val_loss: 0.2691 - val_acc: 0.9050 - lr: 0.0030\n","Epoch 3/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8898\n","Epoch 3: val_acc improved from 0.90500 to 0.90675, saving model to cv_study10.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3220 - acc: 0.8898 - val_loss: 0.2598 - val_acc: 0.9068 - lr: 0.0030\n","Epoch 4/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.8995\n","Epoch 4: val_acc improved from 0.90675 to 0.91075, saving model to cv_study10.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2954 - acc: 0.8996 - val_loss: 0.2464 - val_acc: 0.9107 - lr: 0.0030\n","Epoch 5/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9052\n","Epoch 5: val_acc improved from 0.91075 to 0.91275, saving model to cv_study10.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2772 - acc: 0.9053 - val_loss: 0.2428 - val_acc: 0.9128 - lr: 0.0030\n","Epoch 6/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9105\n","Epoch 6: val_acc did not improve from 0.91275\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2624 - acc: 0.9105 - val_loss: 0.2369 - val_acc: 0.9128 - lr: 0.0030\n","Epoch 7/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9152\n","Epoch 7: val_acc improved from 0.91275 to 0.91450, saving model to cv_study10.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2489 - acc: 0.9152 - val_loss: 0.2417 - val_acc: 0.9145 - lr: 0.0030\n","Epoch 8/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9205\n","Epoch 8: val_acc improved from 0.91450 to 0.93175, saving model to cv_study10.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2365 - acc: 0.9205 - val_loss: 0.2037 - val_acc: 0.9317 - lr: 0.0030\n","Epoch 9/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9208\n","Epoch 9: val_acc did not improve from 0.93175\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2312 - acc: 0.9209 - val_loss: 0.2146 - val_acc: 0.9258 - lr: 0.0030\n","Epoch 10/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9228\n","Epoch 10: val_acc did not improve from 0.93175\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2233 - acc: 0.9227 - val_loss: 0.2048 - val_acc: 0.9255 - lr: 0.0030\n","Epoch 11/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9260\n","Epoch 11: val_acc did not improve from 0.93175\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.2180 - acc: 0.9260 - val_loss: 0.2032 - val_acc: 0.9317 - lr: 0.0030\n","Epoch 12/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9297\n","Epoch 12: val_acc improved from 0.93175 to 0.93575, saving model to cv_study10.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2105 - acc: 0.9297 - val_loss: 0.1928 - val_acc: 0.9358 - lr: 0.0030\n","Epoch 13/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9316\n","Epoch 13: val_acc did not improve from 0.93575\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2016 - acc: 0.9316 - val_loss: 0.1976 - val_acc: 0.9337 - lr: 0.0030\n","Epoch 14/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1981 - acc: 0.9332\n","Epoch 14: val_acc improved from 0.93575 to 0.93675, saving model to cv_study10.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1981 - acc: 0.9332 - val_loss: 0.1883 - val_acc: 0.9367 - lr: 0.0030\n","Epoch 15/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9343\n","Epoch 15: val_acc did not improve from 0.93675\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1960 - acc: 0.9343 - val_loss: 0.1971 - val_acc: 0.9327 - lr: 0.0030\n","Epoch 16/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1888 - acc: 0.9368\n","Epoch 16: val_acc did not improve from 0.93675\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1888 - acc: 0.9368 - val_loss: 0.2063 - val_acc: 0.9310 - lr: 0.0030\n","Epoch 17/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9366\n","Epoch 17: val_acc did not improve from 0.93675\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1865 - acc: 0.9366 - val_loss: 0.2034 - val_acc: 0.9360 - lr: 0.0030\n","Epoch 18/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9383\n","Epoch 18: val_acc improved from 0.93675 to 0.93975, saving model to cv_study10.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1834 - acc: 0.9384 - val_loss: 0.1927 - val_acc: 0.9398 - lr: 0.0030\n","Epoch 19/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9390\n","Epoch 19: val_acc did not improve from 0.93975\n","\n","Epoch 19: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1795 - acc: 0.9390 - val_loss: 0.1983 - val_acc: 0.9362 - lr: 0.0030\n","Epoch 20/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1550 - acc: 0.9478\n","Epoch 20: val_acc improved from 0.93975 to 0.94100, saving model to cv_study10.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1550 - acc: 0.9478 - val_loss: 0.1810 - val_acc: 0.9410 - lr: 0.0015\n","Epoch 21/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9521\n","Epoch 21: val_acc did not improve from 0.94100\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1453 - acc: 0.9522 - val_loss: 0.1967 - val_acc: 0.9367 - lr: 0.0015\n","Epoch 22/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1439 - acc: 0.9521\n","Epoch 22: val_acc improved from 0.94100 to 0.94750, saving model to cv_study10.h5\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1439 - acc: 0.9521 - val_loss: 0.1819 - val_acc: 0.9475 - lr: 0.0015\n","Epoch 23/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9555\n","Epoch 23: val_acc did not improve from 0.94750\n","1750/1750 [==============================] - 19s 11ms/step - loss: 0.1364 - acc: 0.9554 - val_loss: 0.1751 - val_acc: 0.9452 - lr: 0.0015\n","Epoch 24/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9550\n","Epoch 24: val_acc did not improve from 0.94750\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1336 - acc: 0.9549 - val_loss: 0.1884 - val_acc: 0.9402 - lr: 0.0015\n","Epoch 25/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1341 - acc: 0.9554\n","Epoch 25: val_acc did not improve from 0.94750\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1341 - acc: 0.9554 - val_loss: 0.1821 - val_acc: 0.9470 - lr: 0.0015\n","Epoch 26/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1326 - acc: 0.9552\n","Epoch 26: val_acc did not improve from 0.94750\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1326 - acc: 0.9552 - val_loss: 0.2050 - val_acc: 0.9405 - lr: 0.0015\n","Epoch 27/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9574\n","Epoch 27: val_acc did not improve from 0.94750\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1281 - acc: 0.9574 - val_loss: 0.1908 - val_acc: 0.9448 - lr: 0.0015\n","Epoch 27: early stopping\n","10번째 학습 FOLD 학습 완료\n","\n","=========================\n","11번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.5621 - acc: 0.8008\n","Epoch 1: val_acc improved from -inf to 0.87600, saving model to cv_study11.h5\n","1750/1750 [==============================] - 22s 11ms/step - loss: 0.5617 - acc: 0.8009 - val_loss: 0.3483 - val_acc: 0.8760 - lr: 0.0030\n","Epoch 2/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8771\n","Epoch 2: val_acc improved from 0.87600 to 0.89050, saving model to cv_study11.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3567 - acc: 0.8772 - val_loss: 0.3245 - val_acc: 0.8905 - lr: 0.0030\n","Epoch 3/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.3117 - acc: 0.8929\n","Epoch 3: val_acc did not improve from 0.89050\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3117 - acc: 0.8929 - val_loss: 0.3064 - val_acc: 0.8900 - lr: 0.0030\n","Epoch 4/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9012\n","Epoch 4: val_acc improved from 0.89050 to 0.90750, saving model to cv_study11.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2869 - acc: 0.9012 - val_loss: 0.2618 - val_acc: 0.9075 - lr: 0.0030\n","Epoch 5/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9071\n","Epoch 5: val_acc improved from 0.90750 to 0.91550, saving model to cv_study11.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2701 - acc: 0.9071 - val_loss: 0.2321 - val_acc: 0.9155 - lr: 0.0030\n","Epoch 6/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9145\n","Epoch 6: val_acc improved from 0.91550 to 0.91825, saving model to cv_study11.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2501 - acc: 0.9145 - val_loss: 0.2263 - val_acc: 0.9183 - lr: 0.0030\n","Epoch 7/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2393 - acc: 0.9185\n","Epoch 7: val_acc did not improve from 0.91825\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2393 - acc: 0.9185 - val_loss: 0.2371 - val_acc: 0.9130 - lr: 0.0030\n","Epoch 8/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9219\n","Epoch 8: val_acc improved from 0.91825 to 0.92300, saving model to cv_study11.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2332 - acc: 0.9218 - val_loss: 0.2182 - val_acc: 0.9230 - lr: 0.0030\n","Epoch 9/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9235\n","Epoch 9: val_acc did not improve from 0.92300\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2222 - acc: 0.9235 - val_loss: 0.2141 - val_acc: 0.9230 - lr: 0.0030\n","Epoch 10/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2178 - acc: 0.9271\n","Epoch 10: val_acc improved from 0.92300 to 0.92850, saving model to cv_study11.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2178 - acc: 0.9271 - val_loss: 0.2098 - val_acc: 0.9285 - lr: 0.0030\n","Epoch 11/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9287\n","Epoch 11: val_acc did not improve from 0.92850\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2098 - acc: 0.9287 - val_loss: 0.2146 - val_acc: 0.9285 - lr: 0.0030\n","Epoch 12/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9318\n","Epoch 12: val_acc did not improve from 0.92850\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2027 - acc: 0.9318 - val_loss: 0.2177 - val_acc: 0.9285 - lr: 0.0030\n","Epoch 13/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9334\n","Epoch 13: val_acc improved from 0.92850 to 0.93250, saving model to cv_study11.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1976 - acc: 0.9334 - val_loss: 0.2037 - val_acc: 0.9325 - lr: 0.0030\n","Epoch 14/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9357\n","Epoch 14: val_acc did not improve from 0.93250\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1940 - acc: 0.9357 - val_loss: 0.2361 - val_acc: 0.9243 - lr: 0.0030\n","Epoch 15/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9368\n","Epoch 15: val_acc did not improve from 0.93250\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1898 - acc: 0.9368 - val_loss: 0.2213 - val_acc: 0.9245 - lr: 0.0030\n","Epoch 16/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1844 - acc: 0.9383\n","Epoch 16: val_acc did not improve from 0.93250\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1844 - acc: 0.9383 - val_loss: 0.2147 - val_acc: 0.9273 - lr: 0.0030\n","Epoch 17/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9395\n","Epoch 17: val_acc improved from 0.93250 to 0.93300, saving model to cv_study11.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1812 - acc: 0.9395 - val_loss: 0.2069 - val_acc: 0.9330 - lr: 0.0030\n","Epoch 18/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1774 - acc: 0.9412\n","Epoch 18: val_acc did not improve from 0.93300\n","\n","Epoch 18: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1774 - acc: 0.9412 - val_loss: 0.2330 - val_acc: 0.9222 - lr: 0.0030\n","Epoch 19/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9487\n","Epoch 19: val_acc improved from 0.93300 to 0.93700, saving model to cv_study11.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1532 - acc: 0.9487 - val_loss: 0.2171 - val_acc: 0.9370 - lr: 0.0015\n","Epoch 20/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1496 - acc: 0.9507\n","Epoch 20: val_acc did not improve from 0.93700\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1496 - acc: 0.9507 - val_loss: 0.2079 - val_acc: 0.9348 - lr: 0.0015\n","Epoch 21/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9523\n","Epoch 21: val_acc did not improve from 0.93700\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1422 - acc: 0.9522 - val_loss: 0.2006 - val_acc: 0.9360 - lr: 0.0015\n","Epoch 22/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9544\n","Epoch 22: val_acc improved from 0.93700 to 0.93725, saving model to cv_study11.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1368 - acc: 0.9544 - val_loss: 0.2174 - val_acc: 0.9373 - lr: 0.0015\n","Epoch 23/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9563\n","Epoch 23: val_acc improved from 0.93725 to 0.93950, saving model to cv_study11.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1334 - acc: 0.9563 - val_loss: 0.2002 - val_acc: 0.9395 - lr: 0.0015\n","Epoch 24/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1326 - acc: 0.9563\n","Epoch 24: val_acc did not improve from 0.93950\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1326 - acc: 0.9563 - val_loss: 0.2097 - val_acc: 0.9325 - lr: 0.0015\n","Epoch 25/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9575\n","Epoch 25: val_acc did not improve from 0.93950\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1289 - acc: 0.9575 - val_loss: 0.2426 - val_acc: 0.9302 - lr: 0.0015\n","Epoch 26/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9584\n","Epoch 26: val_acc did not improve from 0.93950\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1273 - acc: 0.9583 - val_loss: 0.2136 - val_acc: 0.9345 - lr: 0.0015\n","Epoch 27/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9595\n","Epoch 27: val_acc did not improve from 0.93950\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1237 - acc: 0.9596 - val_loss: 0.2161 - val_acc: 0.9340 - lr: 0.0015\n","Epoch 28/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9605\n","Epoch 28: val_acc did not improve from 0.93950\n","\n","Epoch 28: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1202 - acc: 0.9604 - val_loss: 0.2043 - val_acc: 0.9355 - lr: 0.0015\n","Epoch 28: early stopping\n","11번째 학습 FOLD 학습 완료\n","\n","=========================\n","12번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.5602 - acc: 0.8016\n","Epoch 1: val_acc improved from -inf to 0.87700, saving model to cv_study12.h5\n","1750/1750 [==============================] - 23s 12ms/step - loss: 0.5600 - acc: 0.8016 - val_loss: 0.3591 - val_acc: 0.8770 - lr: 0.0030\n","Epoch 2/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8784\n","Epoch 2: val_acc improved from 0.87700 to 0.89825, saving model to cv_study12.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.3554 - acc: 0.8785 - val_loss: 0.2875 - val_acc: 0.8982 - lr: 0.0030\n","Epoch 3/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.8918\n","Epoch 3: val_acc improved from 0.89825 to 0.90900, saving model to cv_study12.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.3149 - acc: 0.8917 - val_loss: 0.2564 - val_acc: 0.9090 - lr: 0.0030\n","Epoch 4/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9000\n","Epoch 4: val_acc did not improve from 0.90900\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2915 - acc: 0.9000 - val_loss: 0.2719 - val_acc: 0.9068 - lr: 0.0030\n","Epoch 5/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9062\n","Epoch 5: val_acc improved from 0.90900 to 0.91700, saving model to cv_study12.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2742 - acc: 0.9062 - val_loss: 0.2274 - val_acc: 0.9170 - lr: 0.0030\n","Epoch 6/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.9123\n","Epoch 6: val_acc did not improve from 0.91700\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2579 - acc: 0.9124 - val_loss: 0.2275 - val_acc: 0.9168 - lr: 0.0030\n","Epoch 7/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9147\n","Epoch 7: val_acc improved from 0.91700 to 0.92400, saving model to cv_study12.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2487 - acc: 0.9147 - val_loss: 0.2070 - val_acc: 0.9240 - lr: 0.0030\n","Epoch 8/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9172\n","Epoch 8: val_acc improved from 0.92400 to 0.92800, saving model to cv_study12.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2401 - acc: 0.9171 - val_loss: 0.2015 - val_acc: 0.9280 - lr: 0.0030\n","Epoch 9/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2325 - acc: 0.9213\n","Epoch 9: val_acc did not improve from 0.92800\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2325 - acc: 0.9213 - val_loss: 0.2122 - val_acc: 0.9258 - lr: 0.0030\n","Epoch 10/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9243\n","Epoch 10: val_acc improved from 0.92800 to 0.93225, saving model to cv_study12.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2242 - acc: 0.9244 - val_loss: 0.2009 - val_acc: 0.9323 - lr: 0.0030\n","Epoch 11/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9262\n","Epoch 11: val_acc did not improve from 0.93225\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2154 - acc: 0.9262 - val_loss: 0.2199 - val_acc: 0.9210 - lr: 0.0030\n","Epoch 12/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9284\n","Epoch 12: val_acc improved from 0.93225 to 0.93850, saving model to cv_study12.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2105 - acc: 0.9284 - val_loss: 0.1750 - val_acc: 0.9385 - lr: 0.0030\n","Epoch 13/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2033 - acc: 0.9313\n","Epoch 13: val_acc did not improve from 0.93850\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2033 - acc: 0.9313 - val_loss: 0.1867 - val_acc: 0.9373 - lr: 0.0030\n","Epoch 14/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9320\n","Epoch 14: val_acc did not improve from 0.93850\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2024 - acc: 0.9319 - val_loss: 0.2155 - val_acc: 0.9275 - lr: 0.0030\n","Epoch 15/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9351\n","Epoch 15: val_acc did not improve from 0.93850\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1920 - acc: 0.9351 - val_loss: 0.1861 - val_acc: 0.9327 - lr: 0.0030\n","Epoch 16/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9372\n","Epoch 16: val_acc improved from 0.93850 to 0.94200, saving model to cv_study12.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1885 - acc: 0.9371 - val_loss: 0.1682 - val_acc: 0.9420 - lr: 0.0030\n","Epoch 17/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1837 - acc: 0.9382\n","Epoch 17: val_acc improved from 0.94200 to 0.94575, saving model to cv_study12.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1837 - acc: 0.9382 - val_loss: 0.1670 - val_acc: 0.9457 - lr: 0.0030\n","Epoch 18/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9385\n","Epoch 18: val_acc did not improve from 0.94575\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1856 - acc: 0.9385 - val_loss: 0.1664 - val_acc: 0.9408 - lr: 0.0030\n","Epoch 19/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9423\n","Epoch 19: val_acc did not improve from 0.94575\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1713 - acc: 0.9424 - val_loss: 0.1811 - val_acc: 0.9402 - lr: 0.0030\n","Epoch 20/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9433\n","Epoch 20: val_acc did not improve from 0.94575\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1706 - acc: 0.9433 - val_loss: 0.2137 - val_acc: 0.9265 - lr: 0.0030\n","Epoch 21/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9439\n","Epoch 21: val_acc did not improve from 0.94575\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1693 - acc: 0.9439 - val_loss: 0.2009 - val_acc: 0.9287 - lr: 0.0030\n","Epoch 22/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9445\n","Epoch 22: val_acc did not improve from 0.94575\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1653 - acc: 0.9445 - val_loss: 0.1787 - val_acc: 0.9405 - lr: 0.0030\n","Epoch 22: early stopping\n","12번째 학습 FOLD 학습 완료\n","\n","=========================\n","13번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.5496 - acc: 0.8067\n","Epoch 1: val_acc improved from -inf to 0.87775, saving model to cv_study13.h5\n","1750/1750 [==============================] - 23s 12ms/step - loss: 0.5496 - acc: 0.8067 - val_loss: 0.3425 - val_acc: 0.8777 - lr: 0.0030\n","Epoch 2/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8756\n","Epoch 2: val_acc improved from 0.87775 to 0.89900, saving model to cv_study13.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.3543 - acc: 0.8756 - val_loss: 0.2712 - val_acc: 0.8990 - lr: 0.0030\n","Epoch 3/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.8925\n","Epoch 3: val_acc improved from 0.89900 to 0.91100, saving model to cv_study13.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.3148 - acc: 0.8925 - val_loss: 0.2575 - val_acc: 0.9110 - lr: 0.0030\n","Epoch 4/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9022\n","Epoch 4: val_acc improved from 0.91100 to 0.91475, saving model to cv_study13.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2861 - acc: 0.9022 - val_loss: 0.2480 - val_acc: 0.9147 - lr: 0.0030\n","Epoch 5/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.9086\n","Epoch 5: val_acc improved from 0.91475 to 0.92000, saving model to cv_study13.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2686 - acc: 0.9086 - val_loss: 0.2235 - val_acc: 0.9200 - lr: 0.0030\n","Epoch 6/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2554 - acc: 0.9137\n","Epoch 6: val_acc improved from 0.92000 to 0.92275, saving model to cv_study13.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2554 - acc: 0.9137 - val_loss: 0.2239 - val_acc: 0.9227 - lr: 0.0030\n","Epoch 7/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9179\n","Epoch 7: val_acc improved from 0.92275 to 0.92525, saving model to cv_study13.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2423 - acc: 0.9180 - val_loss: 0.2055 - val_acc: 0.9252 - lr: 0.0030\n","Epoch 8/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2325 - acc: 0.9198\n","Epoch 8: val_acc improved from 0.92525 to 0.92725, saving model to cv_study13.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2325 - acc: 0.9198 - val_loss: 0.2073 - val_acc: 0.9273 - lr: 0.0030\n","Epoch 9/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9231\n","Epoch 9: val_acc did not improve from 0.92725\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2288 - acc: 0.9231 - val_loss: 0.2126 - val_acc: 0.9268 - lr: 0.0030\n","Epoch 10/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2178 - acc: 0.9260\n","Epoch 10: val_acc did not improve from 0.92725\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2178 - acc: 0.9260 - val_loss: 0.2818 - val_acc: 0.9090 - lr: 0.0030\n","Epoch 11/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9281\n","Epoch 11: val_acc did not improve from 0.92725\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2118 - acc: 0.9281 - val_loss: 0.2054 - val_acc: 0.9247 - lr: 0.0030\n","Epoch 12/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2027 - acc: 0.9313\n","Epoch 12: val_acc improved from 0.92725 to 0.93150, saving model to cv_study13.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2027 - acc: 0.9313 - val_loss: 0.1912 - val_acc: 0.9315 - lr: 0.0030\n","Epoch 13/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1989 - acc: 0.9324\n","Epoch 13: val_acc improved from 0.93150 to 0.93250, saving model to cv_study13.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1989 - acc: 0.9324 - val_loss: 0.2000 - val_acc: 0.9325 - lr: 0.0030\n","Epoch 14/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9348\n","Epoch 14: val_acc did not improve from 0.93250\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1936 - acc: 0.9347 - val_loss: 0.1967 - val_acc: 0.9270 - lr: 0.0030\n","Epoch 15/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1861 - acc: 0.9362\n","Epoch 15: val_acc improved from 0.93250 to 0.93525, saving model to cv_study13.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1861 - acc: 0.9362 - val_loss: 0.1960 - val_acc: 0.9352 - lr: 0.0030\n","Epoch 16/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9383\n","Epoch 16: val_acc did not improve from 0.93525\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1846 - acc: 0.9383 - val_loss: 0.2026 - val_acc: 0.9295 - lr: 0.0030\n","Epoch 17/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9399\n","Epoch 17: val_acc improved from 0.93525 to 0.93675, saving model to cv_study13.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1784 - acc: 0.9399 - val_loss: 0.1796 - val_acc: 0.9367 - lr: 0.0030\n","Epoch 18/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9407\n","Epoch 18: val_acc improved from 0.93675 to 0.93875, saving model to cv_study13.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1757 - acc: 0.9406 - val_loss: 0.1787 - val_acc: 0.9388 - lr: 0.0030\n","Epoch 19/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9410\n","Epoch 19: val_acc did not improve from 0.93875\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1747 - acc: 0.9409 - val_loss: 0.1890 - val_acc: 0.9362 - lr: 0.0030\n","Epoch 20/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9426\n","Epoch 20: val_acc did not improve from 0.93875\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1692 - acc: 0.9426 - val_loss: 0.1798 - val_acc: 0.9380 - lr: 0.0030\n","Epoch 21/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9450\n","Epoch 21: val_acc did not improve from 0.93875\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1635 - acc: 0.9451 - val_loss: 0.1911 - val_acc: 0.9380 - lr: 0.0030\n","Epoch 22/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9468\n","Epoch 22: val_acc improved from 0.93875 to 0.94200, saving model to cv_study13.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1610 - acc: 0.9468 - val_loss: 0.1816 - val_acc: 0.9420 - lr: 0.0030\n","Epoch 23/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1593 - acc: 0.9478\n","Epoch 23: val_acc improved from 0.94200 to 0.94275, saving model to cv_study13.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1593 - acc: 0.9478 - val_loss: 0.1717 - val_acc: 0.9427 - lr: 0.0030\n","Epoch 24/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9484\n","Epoch 24: val_acc did not improve from 0.94275\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1547 - acc: 0.9484 - val_loss: 0.1758 - val_acc: 0.9400 - lr: 0.0030\n","Epoch 25/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9501\n","Epoch 25: val_acc did not improve from 0.94275\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1528 - acc: 0.9501 - val_loss: 0.1879 - val_acc: 0.9395 - lr: 0.0030\n","Epoch 26/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9498\n","Epoch 26: val_acc did not improve from 0.94275\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1504 - acc: 0.9498 - val_loss: 0.1942 - val_acc: 0.9395 - lr: 0.0030\n","Epoch 27/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1478 - acc: 0.9516\n","Epoch 27: val_acc did not improve from 0.94275\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1478 - acc: 0.9516 - val_loss: 0.1860 - val_acc: 0.9423 - lr: 0.0030\n","Epoch 28/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9528\n","Epoch 28: val_acc did not improve from 0.94275\n","\n","Epoch 28: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1452 - acc: 0.9528 - val_loss: 0.1837 - val_acc: 0.9392 - lr: 0.0030\n","Epoch 28: early stopping\n","13번째 학습 FOLD 학습 완료\n","\n","=========================\n","14번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.8023\n","Epoch 1: val_acc improved from -inf to 0.87650, saving model to cv_study14.h5\n","1750/1750 [==============================] - 23s 12ms/step - loss: 0.5549 - acc: 0.8024 - val_loss: 0.3599 - val_acc: 0.8765 - lr: 0.0030\n","Epoch 2/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8780\n","Epoch 2: val_acc improved from 0.87650 to 0.89525, saving model to cv_study14.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.3565 - acc: 0.8781 - val_loss: 0.2944 - val_acc: 0.8953 - lr: 0.0030\n","Epoch 3/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.8932\n","Epoch 3: val_acc improved from 0.89525 to 0.91250, saving model to cv_study14.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.3141 - acc: 0.8932 - val_loss: 0.2514 - val_acc: 0.9125 - lr: 0.0030\n","Epoch 4/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9006\n","Epoch 4: val_acc improved from 0.91250 to 0.91725, saving model to cv_study14.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2943 - acc: 0.9006 - val_loss: 0.2428 - val_acc: 0.9172 - lr: 0.0030\n","Epoch 5/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9064\n","Epoch 5: val_acc did not improve from 0.91725\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2740 - acc: 0.9064 - val_loss: 0.2370 - val_acc: 0.9172 - lr: 0.0030\n","Epoch 6/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2629 - acc: 0.9106\n","Epoch 6: val_acc improved from 0.91725 to 0.92075, saving model to cv_study14.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2627 - acc: 0.9106 - val_loss: 0.2168 - val_acc: 0.9208 - lr: 0.0030\n","Epoch 7/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2495 - acc: 0.9159\n","Epoch 7: val_acc improved from 0.92075 to 0.92525, saving model to cv_study14.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2493 - acc: 0.9160 - val_loss: 0.2195 - val_acc: 0.9252 - lr: 0.0030\n","Epoch 8/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2416 - acc: 0.9182\n","Epoch 8: val_acc improved from 0.92525 to 0.92550, saving model to cv_study14.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2416 - acc: 0.9182 - val_loss: 0.2151 - val_acc: 0.9255 - lr: 0.0030\n","Epoch 9/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2296 - acc: 0.9237\n","Epoch 9: val_acc improved from 0.92550 to 0.93150, saving model to cv_study14.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2296 - acc: 0.9237 - val_loss: 0.2027 - val_acc: 0.9315 - lr: 0.0030\n","Epoch 10/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9245\n","Epoch 10: val_acc did not improve from 0.93150\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2250 - acc: 0.9245 - val_loss: 0.2056 - val_acc: 0.9268 - lr: 0.0030\n","Epoch 11/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9278\n","Epoch 11: val_acc did not improve from 0.93150\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2170 - acc: 0.9277 - val_loss: 0.2089 - val_acc: 0.9295 - lr: 0.0030\n","Epoch 12/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9305\n","Epoch 12: val_acc improved from 0.93150 to 0.93625, saving model to cv_study14.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2076 - acc: 0.9305 - val_loss: 0.1929 - val_acc: 0.9362 - lr: 0.0030\n","Epoch 13/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2043 - acc: 0.9319\n","Epoch 13: val_acc did not improve from 0.93625\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2043 - acc: 0.9319 - val_loss: 0.1959 - val_acc: 0.9342 - lr: 0.0030\n","Epoch 14/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9337\n","Epoch 14: val_acc improved from 0.93625 to 0.93950, saving model to cv_study14.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.2016 - acc: 0.9336 - val_loss: 0.1868 - val_acc: 0.9395 - lr: 0.0030\n","Epoch 15/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9338\n","Epoch 15: val_acc did not improve from 0.93950\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1950 - acc: 0.9338 - val_loss: 0.1878 - val_acc: 0.9367 - lr: 0.0030\n","Epoch 16/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9381\n","Epoch 16: val_acc did not improve from 0.93950\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1855 - acc: 0.9381 - val_loss: 0.2005 - val_acc: 0.9383 - lr: 0.0030\n","Epoch 17/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9378\n","Epoch 17: val_acc did not improve from 0.93950\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1860 - acc: 0.9377 - val_loss: 0.2001 - val_acc: 0.9317 - lr: 0.0030\n","Epoch 18/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9391\n","Epoch 18: val_acc improved from 0.93950 to 0.94225, saving model to cv_study14.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1791 - acc: 0.9391 - val_loss: 0.1865 - val_acc: 0.9423 - lr: 0.0030\n","Epoch 19/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9420\n","Epoch 19: val_acc did not improve from 0.94225\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1723 - acc: 0.9420 - val_loss: 0.1944 - val_acc: 0.9385 - lr: 0.0030\n","Epoch 20/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9416\n","Epoch 20: val_acc did not improve from 0.94225\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1769 - acc: 0.9416 - val_loss: 0.2075 - val_acc: 0.9365 - lr: 0.0030\n","Epoch 21/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1692 - acc: 0.9443\n","Epoch 21: val_acc did not improve from 0.94225\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1692 - acc: 0.9443 - val_loss: 0.2022 - val_acc: 0.9388 - lr: 0.0030\n","Epoch 22/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1687 - acc: 0.9435\n","Epoch 22: val_acc did not improve from 0.94225\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1687 - acc: 0.9435 - val_loss: 0.2016 - val_acc: 0.9367 - lr: 0.0030\n","Epoch 23/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9458\n","Epoch 23: val_acc did not improve from 0.94225\n","\n","Epoch 23: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1643 - acc: 0.9459 - val_loss: 0.2190 - val_acc: 0.9348 - lr: 0.0030\n","Epoch 23: early stopping\n","14번째 학습 FOLD 학습 완료\n","\n","=========================\n","15번째 학습 FOLD 학습 시작\n","Epoch 1/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.5675 - acc: 0.7971\n","Epoch 1: val_acc improved from -inf to 0.88575, saving model to cv_study15.h5\n","1750/1750 [==============================] - 23s 12ms/step - loss: 0.5669 - acc: 0.7973 - val_loss: 0.3186 - val_acc: 0.8857 - lr: 0.0030\n","Epoch 2/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3654 - acc: 0.8748\n","Epoch 2: val_acc improved from 0.88575 to 0.89175, saving model to cv_study15.h5\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.3654 - acc: 0.8748 - val_loss: 0.2965 - val_acc: 0.8917 - lr: 0.0030\n","Epoch 3/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.3187 - acc: 0.8893\n","Epoch 3: val_acc improved from 0.89175 to 0.91425, saving model to cv_study15.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.3187 - acc: 0.8893 - val_loss: 0.2497 - val_acc: 0.9143 - lr: 0.0030\n","Epoch 4/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.8988\n","Epoch 4: val_acc improved from 0.91425 to 0.91700, saving model to cv_study15.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2932 - acc: 0.8989 - val_loss: 0.2399 - val_acc: 0.9170 - lr: 0.0030\n","Epoch 5/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2756 - acc: 0.9053\n","Epoch 5: val_acc improved from 0.91700 to 0.92125, saving model to cv_study15.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2756 - acc: 0.9053 - val_loss: 0.2283 - val_acc: 0.9212 - lr: 0.0030\n","Epoch 6/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9110\n","Epoch 6: val_acc did not improve from 0.92125\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2610 - acc: 0.9110 - val_loss: 0.2466 - val_acc: 0.9118 - lr: 0.0030\n","Epoch 7/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9169\n","Epoch 7: val_acc improved from 0.92125 to 0.93150, saving model to cv_study15.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2449 - acc: 0.9169 - val_loss: 0.2065 - val_acc: 0.9315 - lr: 0.0030\n","Epoch 8/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9191\n","Epoch 8: val_acc did not improve from 0.93150\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2385 - acc: 0.9191 - val_loss: 0.2264 - val_acc: 0.9243 - lr: 0.0030\n","Epoch 9/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9232\n","Epoch 9: val_acc did not improve from 0.93150\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2295 - acc: 0.9232 - val_loss: 0.2107 - val_acc: 0.9250 - lr: 0.0030\n","Epoch 10/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9244\n","Epoch 10: val_acc did not improve from 0.93150\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2237 - acc: 0.9244 - val_loss: 0.2000 - val_acc: 0.9315 - lr: 0.0030\n","Epoch 11/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9273\n","Epoch 11: val_acc did not improve from 0.93150\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2165 - acc: 0.9274 - val_loss: 0.2183 - val_acc: 0.9290 - lr: 0.0030\n","Epoch 12/1000\n","1747/1750 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9292\n","Epoch 12: val_acc improved from 0.93150 to 0.93300, saving model to cv_study15.h5\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.2074 - acc: 0.9292 - val_loss: 0.2003 - val_acc: 0.9330 - lr: 0.0030\n","Epoch 13/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.2053 - acc: 0.9311\n","Epoch 13: val_acc improved from 0.93300 to 0.93350, saving model to cv_study15.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.2053 - acc: 0.9311 - val_loss: 0.2006 - val_acc: 0.9335 - lr: 0.0030\n","Epoch 14/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9327\n","Epoch 14: val_acc did not improve from 0.93350\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1983 - acc: 0.9327 - val_loss: 0.2192 - val_acc: 0.9315 - lr: 0.0030\n","Epoch 15/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9337\n","Epoch 15: val_acc improved from 0.93350 to 0.93800, saving model to cv_study15.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1950 - acc: 0.9337 - val_loss: 0.1977 - val_acc: 0.9380 - lr: 0.0030\n","Epoch 16/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9375\n","Epoch 16: val_acc did not improve from 0.93800\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1897 - acc: 0.9374 - val_loss: 0.2012 - val_acc: 0.9355 - lr: 0.0030\n","Epoch 17/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9381\n","Epoch 17: val_acc did not improve from 0.93800\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1868 - acc: 0.9381 - val_loss: 0.1947 - val_acc: 0.9380 - lr: 0.0030\n","Epoch 18/1000\n","1748/1750 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9387\n","Epoch 18: val_acc improved from 0.93800 to 0.94325, saving model to cv_study15.h5\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1828 - acc: 0.9386 - val_loss: 0.1946 - val_acc: 0.9433 - lr: 0.0030\n","Epoch 19/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1757 - acc: 0.9411\n","Epoch 19: val_acc did not improve from 0.94325\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1757 - acc: 0.9411 - val_loss: 0.1990 - val_acc: 0.9390 - lr: 0.0030\n","Epoch 20/1000\n","1750/1750 [==============================] - ETA: 0s - loss: 0.1719 - acc: 0.9436\n","Epoch 20: val_acc did not improve from 0.94325\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1719 - acc: 0.9436 - val_loss: 0.2042 - val_acc: 0.9373 - lr: 0.0030\n","Epoch 21/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9450\n","Epoch 21: val_acc did not improve from 0.94325\n","1750/1750 [==============================] - 20s 11ms/step - loss: 0.1686 - acc: 0.9449 - val_loss: 0.1893 - val_acc: 0.9402 - lr: 0.0030\n","Epoch 22/1000\n","1746/1750 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9446\n","Epoch 22: val_acc did not improve from 0.94325\n","1750/1750 [==============================] - 20s 12ms/step - loss: 0.1672 - acc: 0.9446 - val_loss: 0.1979 - val_acc: 0.9400 - lr: 0.0030\n","Epoch 23/1000\n","1749/1750 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9462\n","Epoch 23: val_acc did not improve from 0.94325\n","1750/1750 [==============================] - 21s 12ms/step - loss: 0.1638 - acc: 0.9462 - val_loss: 0.2044 - val_acc: 0.9330 - lr: 0.0030\n","Epoch 23: early stopping\n","15번째 학습 FOLD 학습 완료\n","\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"SKshkoVtvwTQ"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"3dAGh6KwCRKN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646197345334,"user_tz":-540,"elapsed":347,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"c5b45e2f-e390-499e-f66e-a4a63fbfb2e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_25\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_157 (Conv2D)         (None, 28, 28, 64)        640       \n","                                                                 \n"," batch_normalization_210 (Ba  (None, 28, 28, 64)       256       \n"," tchNormalization)                                               \n","                                                                 \n"," dropout_117 (Dropout)       (None, 28, 28, 64)        0         \n","                                                                 \n"," conv2d_158 (Conv2D)         (None, 28, 28, 32)        18464     \n","                                                                 \n"," batch_normalization_211 (Ba  (None, 28, 28, 32)       128       \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_159 (Conv2D)         (None, 28, 28, 32)        25632     \n","                                                                 \n"," batch_normalization_212 (Ba  (None, 28, 28, 32)       128       \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_160 (Conv2D)         (None, 28, 28, 32)        25632     \n","                                                                 \n"," batch_normalization_213 (Ba  (None, 28, 28, 32)       128       \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_161 (Conv2D)         (None, 28, 28, 32)        25632     \n","                                                                 \n"," batch_normalization_214 (Ba  (None, 28, 28, 32)       128       \n"," tchNormalization)                                               \n","                                                                 \n"," max_pooling2d_50 (MaxPoolin  (None, 9, 9, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_118 (Dropout)       (None, 9, 9, 32)          0         \n","                                                                 \n"," conv2d_162 (Conv2D)         (None, 9, 9, 64)          18496     \n","                                                                 \n"," batch_normalization_215 (Ba  (None, 9, 9, 64)         256       \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_163 (Conv2D)         (None, 9, 9, 64)          102464    \n","                                                                 \n"," batch_normalization_216 (Ba  (None, 9, 9, 64)         256       \n"," tchNormalization)                                               \n","                                                                 \n"," max_pooling2d_51 (MaxPoolin  (None, 3, 3, 64)         0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_119 (Dropout)       (None, 3, 3, 64)          0         \n","                                                                 \n"," flatten_24 (Flatten)        (None, 576)               0         \n","                                                                 \n"," dense_77 (Dense)            (None, 128)               73856     \n","                                                                 \n"," batch_normalization_217 (Ba  (None, 128)              512       \n"," tchNormalization)                                               \n","                                                                 \n"," dropout_120 (Dropout)       (None, 128)               0         \n","                                                                 \n"," dense_78 (Dense)            (None, 64)                8256      \n","                                                                 \n"," batch_normalization_218 (Ba  (None, 64)               256       \n"," tchNormalization)                                               \n","                                                                 \n"," dropout_121 (Dropout)       (None, 64)                0         \n","                                                                 \n"," dense_79 (Dense)            (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 301,770\n","Trainable params: 300,746\n","Non-trainable params: 1,024\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["# 4. 답안지 작성"],"metadata":{"id":"mIOQDtrwyWjo"}},{"cell_type":"code","source":["submission['label'] = [np.argmax(x) for x in sub_pred] # 각 클래스별 확률에서 제일 높은 확률의 클래스 할당\n","#여기서 np.argmax는 최댓값의 인덱스 값\n","submission.to_csv('cv_study.csv', index = False)"],"metadata":{"id":"0sUx97qmCRM5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. 신경망 재설정"],"metadata":{"id":"tAQ5PB00yU4N"}},{"cell_type":"code","source":["skf = StratifiedKFold(n_splits = 5, random_state = 11, shuffle = True)\n","reLR = ReduceLROnPlateau(patience = 5,verbose = 1,factor = 0.5) # 학습률 조정\n","es = EarlyStopping(patience = 5, verbose=1, monitor = 'val_acc', mode = 'max') # 학습 조기 종료"],"metadata":{"id":"woJ1raOI6faP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub_pred = np.zeros((test.shape[0], 10))  # (10000, 10)의 0으로 채워진 배열 생성\n","# print(sub_pred)\n","\n","for i, (tr_idx, val_idx) in enumerate(skf.split(X, y)): #여기서 X는 학습 데이터, y는 라벨\n","  print('=' * 300)\n","  print(f'{i + 1}번째 학습 FOLD 학습 시작')\n","#   print(tr_idx, '\\t', val_idx)\n","\n","  # 학습데이터\n","  tr_x, tr_y = X[tr_idx], y[tr_idx]\n","#   print(tr_x, '\\t', tr_y)\n","  # 검증데이터\n","  val_x, val_y = X[val_idx], y[val_idx]\n","#   print(val_x, '\\t', val_y)\n","#   print(val_x.shape, '\\t', val_y.shape)\n","\n","#    체크포인트, 모델 가중치 항상 저장\n","  mc = ModelCheckpoint(f'cv_restudy{i + 1}.h5',save_best_only=True, verbose=1, save_weights_only=True)\n","\n","  model = Sequential()\n","\n","  model.add(Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)))  #Conv2D는 컨벌루션 층, 함수는 렐루\n","  model.add(BatchNormalization()) #각 레이어마다 정규화 하는 레이어를 두어, 변형된 분포가 나오지 않도록 조절하게 하는 것이 배치 정규화\n","  model.add(Dense(64,activation='relu'))  # 은닉층 128개\n","  model.add(BatchNormalization())\n","  model.add(Conv2D(64,(3,3),activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dropout(0.3))\n","  model.add(Conv2D(128,(3,3),activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dense(128,activation='relu'))  # 은닉층 128개\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D((3,3)))\n","#   model.add(Dropout(0.3))  # 30% 노드를 끔\n","  model.add(Dense(128,activation='relu'))  # 은닉층 128개\n","  model.add(BatchNormalization())\n","#   model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \n","#   model.add(BatchNormalization())\n","  model.add(Conv2D(64,(5,5),activation='relu'))\n","  model.add(BatchNormalization())\n","#   model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n","#   model.add(BatchNormalization())\n","#   model.add(MaxPooling2D((3,3)))\n","#   model.add(Dropout(0.3))\n","#   model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n","#   model.add(BatchNormalization())\n","#   model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \n","#   model.add(BatchNormalization())\n","  model.add(MaxPooling2D((3,3)))  # 구역 안에서 최댓값을 뽑아내는 풀링 기법\n","#   model.add(Dropout(0.3))\n","  model.add(Flatten())  # 2차원 배열을 1차원으로 바꿔줌\n","#   model.add(Dropout(0.3))\n","  model.add(Dense(64,activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dense(64,activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dense(32,activation='relu'))\n","  model.add(BatchNormalization())\n","#   model.add(Dropout(0.2))\n","  model.add(Dense(10,activation='softmax'))\n","\n","  model.compile(loss='sparse_categorical_crossentropy', optimizer = RMSprop(lr=0.2), metrics=['acc'])\n","\n","  history = model.fit(tr_x, tr_y, epochs = 500, \n","                      validation_data = (val_x, val_y), callbacks = [es, mc, reLR])\n","\n","  model.load_weights(f'cv_restudy{i + 1}.h5')\n","\n","  pred = model.predict(target) / 5\n","  sub_pred += pred\n","  print(f'{i + 1}번째 학습 FOLD 학습 완료\\n')\n","  print(\"=\"*300)"],"metadata":{"id":"DuXREV3YCRUl","colab":{"base_uri":"https://localhost:8080/","height":416},"executionInfo":{"status":"error","timestamp":1646282559292,"user_tz":-540,"elapsed":5085,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"212f1fea-aa6e-4cb4-dfaa-9c45dfbaabd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" 432/1500 [=======>......................] - ETA: 6:47 - loss: 0.4943 - acc: 0.8696"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-5711d6aa95b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   history = model.fit(tr_x, tr_y, epochs = 500, \n\u001b[0;32m---> 65\u001b[0;31m                       validation_data = (val_x, val_y), callbacks = [es, mc, reLR])\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'cv_restudy{i + 1}.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"twWvBmpv01Pk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission['label'] = [np.argmax(x) for x in sub_pred] # 각 클래스별 확률에서 제일 높은 확률의 클래스 할당\n","#여기서 np.argmax는 최댓값의 인덱스 값\n","submission.to_csv('cv_restudy.csv', index = False)\n","submission"],"metadata":{"id":"0Ky0YZG_01fl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 혼공머신 책 참고"],"metadata":{"id":"s78kh1D4FbSV"}},{"cell_type":"code","source":["train = pd.read_csv('train.csv').iloc[:, 1:]\n","test = pd.read_csv('test.csv').iloc[:, 1:]\n","submission = pd.read_csv('sample_submission.csv')\n","\n","display(train)\n","display(test)\n","display(submission)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"IqChI7wjKMtK","executionInfo":{"status":"ok","timestamp":1646352593075,"user_tz":-540,"elapsed":3931,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"9d757c58-f439-4422-be16-622822a5ebc3"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <div id=\"df-efa9dce6-3936-4fca-9ad3-d84713402250\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>pixel9</th>\n","      <th>...</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","      <th>pixel784</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30</td>\n","      <td>43</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>59995</th>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59996</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>73</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59997</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>160</td>\n","      <td>162</td>\n","      <td>163</td>\n","      <td>135</td>\n","      <td>94</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59998</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59999</th>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>60000 rows × 785 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efa9dce6-3936-4fca-9ad3-d84713402250')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-efa9dce6-3936-4fca-9ad3-d84713402250 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-efa9dce6-3936-4fca-9ad3-d84713402250');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n","0          2       0       0       0       0       0       0       0       0   \n","1          9       0       0       0       0       0       0       0       0   \n","2          6       0       0       0       0       0       0       0       5   \n","3          0       0       0       0       1       2       0       0       0   \n","4          3       0       0       0       0       0       0       0       0   \n","...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n","59995      9       0       0       0       0       0       0       0       0   \n","59996      1       0       0       0       0       0       0       0       0   \n","59997      8       0       0       0       0       0       0       0       0   \n","59998      8       0       0       0       0       0       0       0       0   \n","59999      7       0       0       0       0       0       0       0       0   \n","\n","       pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n","0           0  ...         0         0         0         0         0   \n","1           0  ...         0         0         0         0         0   \n","2           0  ...         0         0         0        30        43   \n","3           0  ...         3         0         0         0         0   \n","4           0  ...         0         0         0         0         0   \n","...       ...  ...       ...       ...       ...       ...       ...   \n","59995       0  ...         0         0         0         0         0   \n","59996       0  ...        73         0         0         0         0   \n","59997       0  ...       160       162       163       135        94   \n","59998       0  ...         0         0         0         0         0   \n","59999       0  ...         0         0         0         0         0   \n","\n","       pixel780  pixel781  pixel782  pixel783  pixel784  \n","0             0         0         0         0         0  \n","1             0         0         0         0         0  \n","2             0         0         0         0         0  \n","3             1         0         0         0         0  \n","4             0         0         0         0         0  \n","...         ...       ...       ...       ...       ...  \n","59995         0         0         0         0         0  \n","59996         0         0         0         0         0  \n","59997         0         0         0         0         0  \n","59998         0         0         0         0         0  \n","59999         0         0         0         0         0  \n","\n","[60000 rows x 785 columns]"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <div id=\"df-0249207f-f23f-4822-8a6c-8909e52d3564\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>pixel9</th>\n","      <th>pixel10</th>\n","      <th>...</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","      <th>pixel784</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>103</td>\n","      <td>87</td>\n","      <td>56</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>14</td>\n","      <td>53</td>\n","      <td>99</td>\n","      <td>17</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>63</td>\n","      <td>53</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>161</td>\n","      <td>...</td>\n","      <td>137</td>\n","      <td>126</td>\n","      <td>140</td>\n","      <td>0</td>\n","      <td>133</td>\n","      <td>224</td>\n","      <td>222</td>\n","      <td>56</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>37</td>\n","      <td>...</td>\n","      <td>32</td>\n","      <td>23</td>\n","      <td>14</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>52</td>\n","      <td>23</td>\n","      <td>28</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>175</td>\n","      <td>172</td>\n","      <td>172</td>\n","      <td>182</td>\n","      <td>199</td>\n","      <td>222</td>\n","      <td>42</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>140</td>\n","      <td>119</td>\n","      <td>103</td>\n","      <td>...</td>\n","      <td>111</td>\n","      <td>95</td>\n","      <td>75</td>\n","      <td>44</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 784 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0249207f-f23f-4822-8a6c-8909e52d3564')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0249207f-f23f-4822-8a6c-8909e52d3564 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0249207f-f23f-4822-8a6c-8909e52d3564');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n","0          0       0       0       0       0       0       0       9       8   \n","1          0       0       0       0       0       0       0       0       0   \n","2          0       0       0       0       0       0      14      53      99   \n","3          0       0       0       0       0       0       0       0       0   \n","4          0       0       0       0       0       0       0       0       0   \n","...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n","9995       0       0       0       0       0       0       0       0       0   \n","9996       0       0       0       0       0       0       0       0       0   \n","9997       0       0       0       0       0       0       0       0       0   \n","9998       0       1       3       0       0       0       0       0       0   \n","9999       0       0       0       0       0       0       0     140     119   \n","\n","      pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n","0           0  ...       103        87        56         0         0   \n","1           0  ...        34         0         0         0         0   \n","2          17  ...         0         0         0         0        63   \n","3         161  ...       137       126       140         0       133   \n","4           0  ...         0         0         0         0         0   \n","...       ...  ...       ...       ...       ...       ...       ...   \n","9995       37  ...        32        23        14        20         0   \n","9996        0  ...         0         0         0         2        52   \n","9997        0  ...       175       172       172       182       199   \n","9998        0  ...         0         0         0         0         0   \n","9999      103  ...       111        95        75        44         1   \n","\n","      pixel780  pixel781  pixel782  pixel783  pixel784  \n","0            0         0         0         0         0  \n","1            0         0         0         0         0  \n","2           53        31         0         0         0  \n","3          224       222        56         0         0  \n","4            0         0         0         0         0  \n","...        ...       ...       ...       ...       ...  \n","9995         0         1         0         0         0  \n","9996        23        28         0         0         0  \n","9997       222        42         0         1         0  \n","9998         1         0         0         0         0  \n","9999         0         0         0         0         0  \n","\n","[10000 rows x 784 columns]"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <div id=\"df-848d6b75-1dc2-49e1-8e0d-ba08a17b569b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>9995</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>9996</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>9997</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>9998</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>9999</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-848d6b75-1dc2-49e1-8e0d-ba08a17b569b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-848d6b75-1dc2-49e1-8e0d-ba08a17b569b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-848d6b75-1dc2-49e1-8e0d-ba08a17b569b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      index  label\n","0         0      0\n","1         1      0\n","2         2      0\n","3         3      0\n","4         4      0\n","...     ...    ...\n","9995   9995      0\n","9996   9996      0\n","9997   9997      0\n","9998   9998      0\n","9999   9999      0\n","\n","[10000 rows x 2 columns]"]},"metadata":{}}]},{"cell_type":"code","source":["from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","\n","train_input = train.iloc[:, :]\n","train_target = train.iloc[:, 0]\n","\n","print(train_input.shape, train_target.shape)\n","\n","train_scaled = np.array(train.drop('label', axis = 1), dtype = 'float32')\n","print(train_scaled)\n","train_scaled = train_scaled.reshape(-1, 28, 28, 1) / 255.0\n","print(train_scaled.shape, train_target.shape)\n","\n","target = np.array(test, dtype='float32')\n","target /= 255\n","target = target.reshape(-1, 28, 28, 1)\n","\n","train_scaled, val_scaled, train_target, val_target = train_test_split(\n","    train_scaled, train_target, test_size=0.2, random_state=102)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ikTbC6wKXIY","executionInfo":{"status":"ok","timestamp":1646353198933,"user_tz":-540,"elapsed":1597,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"818a2050-2abe-4588-eabc-cfe6320497ec"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 785) (60000,)\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","(60000, 28, 28, 1) (60000,)\n"]}]},{"cell_type":"code","source":["# from tensorflow import keras\n","# from sklearn.model_selection import train_test_split\n","\n","# (train_input, train_target), (test_input, test_target) = \\\n","#     keras.datasets.fashion_mnist.load_data()\n","\n","# train_scaled = train_input.reshape(-1, 28, 28, 1) / 255.0\n","# print(train_scaled.shape, train_target.shape)\n","# train_scaled, val_scaled, train_target, val_target = train_test_split(\n","#     train_scaled, train_target, test_size=0.2, random_state=42)"],"metadata":{"id":"7ZP20G8vXL_b","executionInfo":{"status":"ok","timestamp":1646353199503,"user_tz":-540,"elapsed":4,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["model = keras.Sequential()\n","\n","model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', \n","                              padding='same', input_shape=(28,28,1)))\n","model.add(BatchNormalization())\n","model.add(keras.layers.MaxPooling2D(2))\n","model.add(keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu', \n","                              padding='same'))\n","model.add(BatchNormalization())\n","model.add(keras.layers.Conv2D(128, kernel_size=(3,3), activation='relu', \n","                              padding='same'))\n","model.add(BatchNormalization())\n","model.add(keras.layers.Conv2D(64, kernel_size=(5,5), activation='relu', \n","                              padding='same'))\n","model.add(BatchNormalization())\n","model.add(keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', \n","                              padding='same'))\n","model.add(BatchNormalization())\n","model.add(keras.layers.MaxPooling2D(2))\n","model.add(keras.layers.Flatten())\n","model.add(keras.layers.Dense(64, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(keras.layers.Dense(32, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(keras.layers.Dense(20, activation='relu'))\n","model.add(BatchNormalization())\n","# model.add(keras.layers.Dropout(0.2))\n","model.add(keras.layers.Dense(10, activation='softmax'))"],"metadata":{"id":"bDkNHSM_F15R","executionInfo":{"status":"ok","timestamp":1646353248015,"user_tz":-540,"elapsed":47651,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["keras.utils.plot_model(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"l1N_wpomVNpP","executionInfo":{"status":"ok","timestamp":1646353248017,"user_tz":-540,"elapsed":27,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"3920abce-ce7d-45da-bf55-771f0ec714ac"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"image/png":"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\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', \n","              metrics='accuracy')\n","\n","checkpoint_cb = keras.callbacks.ModelCheckpoint('best-cnn-model.h5', \n","                                                save_best_only=True)\n","early_stopping_cb = keras.callbacks.EarlyStopping(patience=2,\n","                                                  restore_best_weights=True)\n","\n","history = model.fit(train_scaled, train_target, epochs=20,\n","                    validation_data=(val_scaled, val_target),\n","                    callbacks=[checkpoint_cb, early_stopping_cb])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUSa4R6QF_pY","executionInfo":{"status":"ok","timestamp":1646353300812,"user_tz":-540,"elapsed":52800,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"b7221622-32e1-4c7a-f4c1-9840c2d26ace"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1500/1500 [==============================] - 12s 7ms/step - loss: 0.4650 - accuracy: 0.8456 - val_loss: 0.4254 - val_accuracy: 0.8556\n","Epoch 2/20\n","1500/1500 [==============================] - 10s 7ms/step - loss: 0.2963 - accuracy: 0.8949 - val_loss: 0.2717 - val_accuracy: 0.8997\n","Epoch 3/20\n","1500/1500 [==============================] - 10s 7ms/step - loss: 0.2580 - accuracy: 0.9098 - val_loss: 0.2507 - val_accuracy: 0.9086\n","Epoch 4/20\n","1500/1500 [==============================] - 10s 7ms/step - loss: 0.2296 - accuracy: 0.9179 - val_loss: 0.2608 - val_accuracy: 0.9039\n","Epoch 5/20\n","1500/1500 [==============================] - 11s 7ms/step - loss: 0.2075 - accuracy: 0.9270 - val_loss: 0.3202 - val_accuracy: 0.8836\n"]}]},{"cell_type":"code","source":["model.evaluate(val_scaled, val_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pd3DQcd8HnWN","executionInfo":{"status":"ok","timestamp":1646352982536,"user_tz":-540,"elapsed":61198,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"683147d2-c432-49d2-e103-1356a167e10e"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["375/375 [==============================] - 1s 3ms/step - loss: 0.2349 - accuracy: 0.9144\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.2349063754081726, 0.9144166707992554]"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["preds = model.predict(target)\n","print(preds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_D16g9UlVn9","executionInfo":{"status":"ok","timestamp":1646298278769,"user_tz":-540,"elapsed":14959,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"4a27397b-d1be-4341-cd5c-11f56e04b406"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[9.9559397e-01 2.2685403e-07 9.8214706e-04 ... 4.8951479e-04\n","  6.7665751e-05 2.6917731e-05]\n"," [8.7972074e-07 9.9997938e-01 8.5686565e-07 ... 1.4563869e-06\n","  9.3709389e-07 8.3320077e-07]\n"," [2.3628304e-02 3.8837412e-04 4.2616335e-01 ... 1.0499885e-05\n","  3.2728261e-04 2.8027853e-05]\n"," ...\n"," [5.7515704e-06 3.6829570e-06 4.6131525e-05 ... 4.5564884e-06\n","  9.9990702e-01 4.5357428e-06]\n"," [5.7211651e-07 3.1853469e-07 1.7562650e-06 ... 1.2591564e-06\n","  9.9998391e-01 4.0578675e-06]\n"," [2.6027060e-06 9.9991643e-01 2.5867059e-06 ... 1.3268125e-07\n","  4.6892990e-05 1.4779348e-05]]\n"]}]},{"cell_type":"code","source":["submission['label'] = [np.argmax(x) for x in preds] # 각 클래스별 확률에서 제일 높은 확률의 클래스 할당\n","#여기서 np.argmax는 최댓값의 인덱스 값\n","submission.to_csv('result2.csv', index = False)\n","submission"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"r24v7tAcllAD","executionInfo":{"status":"ok","timestamp":1646298278771,"user_tz":-540,"elapsed":28,"user":{"displayName":"김인성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15909287218291611857"}},"outputId":"5cb01d1a-6b32-4da0-908e-e1504bef34b2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-4a54f161-bc59-4e19-bd16-2fb379ad5251\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>9995</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>9996</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>9997</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>9998</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>9999</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a54f161-bc59-4e19-bd16-2fb379ad5251')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4a54f161-bc59-4e19-bd16-2fb379ad5251 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4a54f161-bc59-4e19-bd16-2fb379ad5251');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      index  label\n","0         0      0\n","1         1      1\n","2         2      6\n","3         3      2\n","4         4      3\n","...     ...    ...\n","9995   9995      0\n","9996   9996      6\n","9997   9997      8\n","9998   9998      8\n","9999   9999      1\n","\n","[10000 rows x 2 columns]"]},"metadata":{},"execution_count":82}]},{"cell_type":"markdown","source":["## 참고 사이트\n","- https://github.com/rickiepark/hg-mldl/blob/master/8-2.ipynb\n","- https://dacon.io/competitions/open/235594/codeshare/2297?page=1&dtype=recent\n"],"metadata":{"id":"1KMVddjo7cIL"}}]}