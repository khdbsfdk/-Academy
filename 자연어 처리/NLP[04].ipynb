{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP[04].ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOlgeTVz4XslteK7PulcbgF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 네이버 영화 리뷰 감성 분석"],"metadata":{"id":"MSgYp8mCZIuw"}},{"cell_type":"markdown","source":["# 1.데이터 준비와 확인"],"metadata":{"id":"mVUqh26iZRFG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2OlFwJZXgNJ"},"outputs":[],"source":["!pip install konlpy"]},{"cell_type":"code","source":["import pandas as pd\n","import urllib.request\n","import matplotlib.pyplot as plt\n","import re\n","\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from collections import Counter"],"metadata":{"id":"K8fggLOlZPg3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 다운로드\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\") # train\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\") # test\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\") # train + test"],"metadata":{"id":"QYzffroaZyzk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = pd.read_table('ratings_train.txt')\n","test_data = pd.read_table('ratings_test.txt')\n","\n","display(train_data)\n","print()\n","display(test_data)"],"metadata":{"id":"B-GHgxJ1aEIh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"train_data 데이터 갯수 : \", len(train_data), '개')\n","print()\n","print(\"test_data 데이터 갯수 : \", len(test_data), '개')"],"metadata":{"id":"VSs8O5xwaUW7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install konlpy\n","!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git \n","%cd Mecab-ko-for-Google-Colab/\n","!bash install_mecab-ko_on_colab190912.sh\n","%cd ../"],"metadata":{"id":"I17MqzH3abDZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.데이터 로더 구성"],"metadata":{"id":"uQC26TBDcE4w"}},{"cell_type":"code","source":["from konlpy.tag import Mecab\n","\n","tokenizer = Mecab()\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"],"metadata":{"id":"WoNvSDrxawfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_and_remove_stopwords(data, stopwords, tokenizer):\n","    result = []\n","\n","    for sentence in data:\n","        curr_data = []\n","        curr_data = tokenizer.morphs(sentence) # mecab 형태소 분석 tokenizer\n","        print(\"curr_data - \", curr_data)\n","        curr_data = [word for word in curr_data if not word in stopwords] # 불용어 제거\n","        print(\"불용어 curr_data - \", curr_data)\n","        result.append(curr_data)\n","    \n","    return result"],"metadata":{"id":"qrG0oFlYcmuN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_data(train_data, test_data, num_words=10000):\n","    # 중복 제거\n","    train_data.drop_duplicates(subset=['document'], inplace=True)\n","    print(\"중복 제거 train_data - \", train_data)\n","    test_data.drop_duplicates(subset=['document'], inplace=True)\n","\n","    # Nan 결측치 제거\n","    train_data = train_data.dropna(how='any')\n","    print(\"결측치 제거 train_data\", len(train_data))\n","    test_data = test_data.dropna(how='any')\n","\n","    # 함수로 토큰화 및 불용어 제거\n","    x_train = tokenize_and_remove_stopwords(train_data['document'], stopwords, tokenizer)\n","    x_test = tokenize_and_remove_stopwords(test_data['document'], stopwords, tokenizer)\n","\n","    # 단어장 만드는 중.....\n","    words = np.concatenate(x_train).tolist() # 배열을 합쳐서 [idx, val] 형태로 만들어줌\n","    counter = Counter(words)\n","    counter = counter.most_common(10000-4) # 일부 상위 항목의 갯수 구하기\n","    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n","    word_to_index = {word:index for index, word in enumerate(vocab)}\n","\n","    def wordlist_to_indexlist(wordlist):\n","        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n","    \n","    x_train = list(map(wordlist_to_indexlist, x_train))\n","    x_test = list(map(wordlist_to_indexlist, x_test))\n","\n","    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index"],"metadata":{"id":"ZmEo67thdHGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)"],"metadata":{"id":"7Vuy14w-fIGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x_train[10])"],"metadata":{"id":"NRhNGnuHjVLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index_to_word = {index:word for word, index in word_to_index.items()}"],"metadata":{"id":"-M3sbqxtj2fl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다.\n","# 단, 모든 문장은 <BOS>로 시작하는 것을 말합니다.\n","def get_encoded_sentence(sentence, word_to_index):\n","    return [word_to_index['<BOS>']] + [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n","\n","# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해주는 함수입니다.\n","def get_encoded_sentences(sentences, word_to_index):\n","    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n","\n","# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다.\n","def get_decoded_sentence(encoded_sentence, index_to_word):\n","    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])\n","    # [1:]를 통해 <BOS> 제외\n","\n","# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다.\n","def get_decoded_sentenes(encoded_sentences, index_to_word):\n","    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"],"metadata":{"id":"m506mqddkFwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_decoded_sentence(x_train[10], index_to_word)"],"metadata":{"id":"3dqtpMPkla8P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.모델 구성을 위한 데이터 분석 및 가공"],"metadata":{"id":"xFWcFMV3lcrk"}},{"cell_type":"code","source":["# 데이터셋 내 문장 길이 분포\n","total_data_text = list(x_train) + list(x_test)\n","\n","# 텍스트데이터 문장길이의 리스트를 생성한 후\n","num_tokens = [len(tokens) for tokens in total_data_text]\n","num_tokens = np.array(num_tokens)\n","\n","# 문장 길이의 평균값, 최대값, 표준편차를 계산\n","print('문장길이 평균 :', np.mean(num_tokens))\n","print('문장길이 최대 :', np.max(num_tokens))\n","print('문장길이 표준편차 :', np.std(num_tokens))\n","\n","# 예를 들어 최대 길이를 (평균 + 2*표준편차)로 한다면,\n","max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n","\n","maxlen = int(max_tokens)\n","print('pad_sequences maxlen : ', maxlen)\n","print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)/ len(num_tokens)))"],"metadata":{"id":"bJ-Ht1DFlbTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 패딩 추가\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, value=word_to_index['<PAD>'], padding='pre', maxlen = maxlen)\n","x_test = keras.preprocessing.sequence.pad_sequences(x_test, value=word_to_index['<PAD>'], padding='pre', maxlen=maxlen)\n","\n","print(x_train.shape)\n","print(x_test.shape)"],"metadata":{"id":"6AA1o670mbD-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.모델 구성 및 validation 구성"],"metadata":{"id":"WNjLYdbonVme"}},{"cell_type":"code","source":["vocab_size = 10000\n","word_vector_dim = 200\n","\n","model = keras.Sequential()\n","model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n","model.add(keras.layers.LSTM(32))\n","model.add(keras.layers.Dense(8, activation='relu'))\n","model.add(keras.layers.Dense(1, activation='sigmoid'))\n","model.summary()"],"metadata":{"id":"LvnZ2kC4nHaC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5.모델 훈련"],"metadata":{"id":"UX2eQZDfpWXw"}},{"cell_type":"code","source":["x_val = x_train[:50000]\n","y_val = y_train[:50000]\n","\n","partial_x_train = x_train[50000:]\n","partial_y_train = y_train[50000:]\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","epochs = 100\n","\n","history = model.fit(partial_x_train,\n","                    partial_y_train,\n","                    epochs=epochs,\n","                    batch_size=512,\n","                    validation_data=(x_val, y_val),\n","                    verbose=1)"],"metadata":{"id":"VNj1ijlJpVN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = model.evaluate(x_test, y_test, verbose=2)\n","\n","print(results)"],"metadata":{"id":"V8cjU8YnqWY1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.loss, accuracy 그래프 시각화"],"metadata":{"id":"xFs54tKkxzwt"}},{"cell_type":"code","source":["history_dict = history.history\n","print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"],"metadata":{"id":"1rNe4a_WwwS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history_dict['accuracy']\n","val_acc = history_dict['val_accuracy']\n","loss = history_dict['loss']\n","val_loss = history_dict['val_loss']"],"metadata":{"id":"VfSHvgd2xEba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = range(1, len(acc)+1)\n","\n","plt.plot(epochs, loss, 'r-', label= 'Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"JWdR5U8YxRc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.clf() #그림을 초기화\n","\n","plt.plot(epochs, acc, 'r-', label= 'Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"rN9pk4WBxxZZ"},"execution_count":null,"outputs":[]}]}